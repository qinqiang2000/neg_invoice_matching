#!/usr/bin/env python3
"""
æ•°æ®åº“æ€§èƒ½è¯Šæ–­è„šæœ¬

æ‰§è¡Œæ•°æ®åº“æ€§èƒ½åˆ†æï¼Œæ”¶é›†å…³é”®æŒ‡æ ‡ï¼Œç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
"""

import sys
import os
import psutil
import time
from datetime import datetime
from typing import Dict, List, Any
import json

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

from core.db_manager import DatabaseManager
from config.config import get_db_config


class PerformanceDiagnostics:
    """æ•°æ®åº“æ€§èƒ½è¯Šæ–­å·¥å…·"""

    def __init__(self, db_config: Dict):
        self.db_config = db_config
        self.db_manager = DatabaseManager(db_config)
        self.diagnosis_results = {}

    def run_full_diagnosis(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½è¯Šæ–­"""
        print("=== å¼€å§‹æ•°æ®åº“æ€§èƒ½è¯Šæ–­ ===\n")

        # 1. åŸºæœ¬æ•°æ®ç»Ÿè®¡
        print("1. æ”¶é›†åŸºæœ¬æ•°æ®ç»Ÿè®¡...")
        self.diagnosis_results['data_stats'] = self._collect_data_stats()

        # 2. ç´¢å¼•ä½¿ç”¨åˆ†æ
        print("2. åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ...")
        self.diagnosis_results['index_analysis'] = self._analyze_indexes()

        # 3. æŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•
        print("3. æ‰§è¡ŒæŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        self.diagnosis_results['query_benchmarks'] = self._run_query_benchmarks()

        # 4. å­˜å‚¨åˆ†æ
        print("4. åˆ†æå­˜å‚¨ä½¿ç”¨æƒ…å†µ...")
        self.diagnosis_results['storage_analysis'] = self._analyze_storage()

        # 5. è¿æ¥å’Œèµ„æºåˆ†æ
        print("5. åˆ†æè¿æ¥å’Œç³»ç»Ÿèµ„æº...")
        self.diagnosis_results['resource_analysis'] = self._analyze_resources()

        # 6. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
        print("6. ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š...")
        report = self._generate_diagnosis_report()

        print("\n=== æ€§èƒ½è¯Šæ–­å®Œæˆ ===")
        return {
            'raw_data': self.diagnosis_results,
            'report': report,
            'timestamp': datetime.now().isoformat()
        }

    def _collect_data_stats(self) -> Dict[str, Any]:
        """æ”¶é›†åŸºæœ¬æ•°æ®ç»Ÿè®¡"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                stats = {}

                # è¡¨æ•°æ®é‡ç»Ÿè®¡
                cur.execute("""
                    SELECT
                        COUNT(*) as total_count,
                        COUNT(*) FILTER (WHERE remaining > 0) as available_count,
                        COUNT(*) FILTER (WHERE remaining = 0) as used_count,
                        ROUND(AVG(remaining::numeric), 2) as avg_remaining,
                        MIN(remaining) as min_remaining,
                        MAX(remaining) as max_remaining
                    FROM blue_lines
                """)
                blue_lines_stats = cur.fetchone()
                stats['blue_lines'] = {
                    'total_count': blue_lines_stats[0],
                    'available_count': blue_lines_stats[1],
                    'used_count': blue_lines_stats[2],
                    'avg_remaining': float(blue_lines_stats[3]) if blue_lines_stats[3] else 0,
                    'min_remaining': float(blue_lines_stats[4]) if blue_lines_stats[4] else 0,
                    'max_remaining': float(blue_lines_stats[5]) if blue_lines_stats[5] else 0
                }

                # æ•°æ®åˆ†å¸ƒåˆ†æ
                cur.execute("""
                    SELECT
                        COUNT(DISTINCT buyer_id) as unique_buyers,
                        COUNT(DISTINCT seller_id) as unique_sellers,
                        COUNT(DISTINCT tax_rate) as unique_tax_rates,
                        COUNT(DISTINCT (buyer_id, seller_id, tax_rate)) as unique_combinations
                    FROM blue_lines
                """)
                distribution = cur.fetchone()
                stats['distribution'] = {
                    'unique_buyers': distribution[0],
                    'unique_sellers': distribution[1],
                    'unique_tax_rates': distribution[2],
                    'unique_combinations': distribution[3],
                    'avg_records_per_combination': blue_lines_stats[0] / distribution[3] if distribution[3] > 0 else 0
                }

                return stats
        finally:
            self.db_manager.pool.putconn(conn)

    def _analyze_indexes(self) -> Dict[str, Any]:
        """åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                # ç´¢å¼•ä½¿ç”¨ç»Ÿè®¡
                cur.execute("""
                    SELECT
                        indexrelname as indexname,
                        idx_scan,
                        idx_tup_read,
                        idx_tup_fetch
                    FROM pg_stat_user_indexes
                    WHERE relname = 'blue_lines'
                    ORDER BY idx_scan DESC
                """)
                index_stats = []
                for row in cur.fetchall():
                    index_stats.append({
                        'indexname': row[0],
                        'idx_scan': row[1],
                        'idx_tup_read': row[2],
                        'idx_tup_fetch': row[3]
                    })

                # ç´¢å¼•å®šä¹‰
                cur.execute("""
                    SELECT indexname, indexdef
                    FROM pg_indexes
                    WHERE tablename = 'blue_lines'
                    ORDER BY indexname
                """)
                index_definitions = []
                for row in cur.fetchall():
                    index_definitions.append({
                        'indexname': row[0],
                        'definition': row[1]
                    })

                return {
                    'usage_stats': index_stats,
                    'definitions': index_definitions
                }
        finally:
            self.db_manager.pool.putconn(conn)

    def _run_query_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡ŒæŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                benchmarks = {}

                # æµ‹è¯•åŸºæœ¬è¿‡æ»¤æŸ¥è¯¢
                test_params = {
                    'tax_rate': 13,  # tax_rate æ˜¯ SMALLINTï¼Œä½¿ç”¨æ•´æ•°
                    'buyer_id': 1,
                    'seller_id': 1
                }

                # 1. åŸºæœ¬è®¡æ•°æŸ¥è¯¢
                start_time = time.perf_counter()
                cur.execute("""
                    SELECT COUNT(*)
                    FROM blue_lines
                    WHERE tax_rate = %s AND buyer_id = %s AND seller_id = %s AND remaining > 0
                """, (test_params['tax_rate'], test_params['buyer_id'], test_params['seller_id']))
                count_result = cur.fetchone()[0]
                count_time = (time.perf_counter() - start_time) * 1000

                benchmarks['basic_count'] = {
                    'duration_ms': round(count_time, 2),
                    'result_count': count_result
                }

                # 2. æ’åºæŸ¥è¯¢ï¼ˆæ¨¡æ‹Ÿå®é™…ä¸šåŠ¡æŸ¥è¯¢ï¼‰
                start_time = time.perf_counter()
                cur.execute("""
                    SELECT line_id, remaining, tax_rate, buyer_id, seller_id
                    FROM blue_lines
                    WHERE tax_rate = %s AND buyer_id = %s AND seller_id = %s AND remaining > 0
                    ORDER BY remaining ASC
                    LIMIT 100
                """, (test_params['tax_rate'], test_params['buyer_id'], test_params['seller_id']))
                sorted_results = cur.fetchall()
                sort_time = (time.perf_counter() - start_time) * 1000

                benchmarks['sorted_query'] = {
                    'duration_ms': round(sort_time, 2),
                    'result_count': len(sorted_results)
                }

                # 3. æ‰§è¡Œè®¡åˆ’åˆ†æ
                cur.execute("""
                    EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON)
                    SELECT line_id, remaining, tax_rate, buyer_id, seller_id
                    FROM blue_lines
                    WHERE tax_rate = %s AND buyer_id = %s AND seller_id = %s AND remaining > 0
                    ORDER BY remaining ASC
                    LIMIT 100
                """, (test_params['tax_rate'], test_params['buyer_id'], test_params['seller_id']))
                explain_result = cur.fetchone()[0]

                benchmarks['explain_analysis'] = explain_result[0]
                benchmarks['test_parameters'] = test_params

                return benchmarks
        finally:
            self.db_manager.pool.putconn(conn)

    def _analyze_storage(self) -> Dict[str, Any]:
        """åˆ†æå­˜å‚¨ä½¿ç”¨æƒ…å†µ"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT
                        tablename,
                        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as total_size,
                        pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,
                        pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size,
                        pg_total_relation_size(schemaname||'.'||tablename) as total_bytes
                    FROM pg_tables
                    WHERE tablename IN ('blue_lines', 'match_records')
                    ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC
                """)

                storage_info = []
                for row in cur.fetchall():
                    storage_info.append({
                        'tablename': row[0],
                        'total_size': row[1],
                        'table_size': row[2],
                        'index_size': row[3],
                        'total_bytes': row[4]
                    })

                return {'table_sizes': storage_info}
        finally:
            self.db_manager.pool.putconn(conn)

    def _analyze_resources(self) -> Dict[str, Any]:
        """åˆ†æç³»ç»Ÿèµ„æºå’Œè¿æ¥æƒ…å†µ"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                # æ•°æ®åº“è¿æ¥çŠ¶æ€
                cur.execute("""
                    SELECT
                        state,
                        COUNT(*) as connection_count
                    FROM pg_stat_activity
                    WHERE datname = current_database()
                    GROUP BY state
                """)
                connection_stats = []
                for row in cur.fetchall():
                    connection_stats.append({
                        'state': row[0],
                        'count': row[1]
                    })

                # ç³»ç»Ÿèµ„æº
                system_resources = {
                    'cpu_percent': psutil.cpu_percent(interval=1),
                    'memory_percent': psutil.virtual_memory().percent,
                    'memory_available_mb': psutil.virtual_memory().available / 1024 / 1024
                }

                return {
                    'database_connections': connection_stats,
                    'system_resources': system_resources
                }
        finally:
            self.db_manager.pool.putconn(conn)

    def _generate_diagnosis_report(self) -> str:
        """ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š"""
        results = self.diagnosis_results
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        report = f"""
# æ•°æ®åº“æ€§èƒ½è¯Šæ–­æŠ¥å‘Š

**è¯Šæ–­æ—¶é—´**: {timestamp}

## 1. æ•°æ®æ¦‚å†µ

### è¡¨æ•°æ®ç»Ÿè®¡
- **æ€»è®°å½•æ•°**: {results['data_stats']['blue_lines']['total_count']:,}
- **å¯ç”¨è®°å½•æ•°**: {results['data_stats']['blue_lines']['available_count']:,}
- **å·²ä½¿ç”¨è®°å½•æ•°**: {results['data_stats']['blue_lines']['used_count']:,}
- **å¯ç”¨ç‡**: {results['data_stats']['blue_lines']['available_count'] / results['data_stats']['blue_lines']['total_count'] * 100:.1f}%

### æ•°æ®åˆ†å¸ƒ
- **ä¹°æ–¹æ•°é‡**: {results['data_stats']['distribution']['unique_buyers']}
- **å–æ–¹æ•°é‡**: {results['data_stats']['distribution']['unique_sellers']}
- **ç¨ç‡ç§ç±»**: {results['data_stats']['distribution']['unique_tax_rates']}
- **ç»„åˆæ€»æ•°**: {results['data_stats']['distribution']['unique_combinations']}
- **å¹³å‡æ¯ç»„åˆè®°å½•æ•°**: {results['data_stats']['distribution']['avg_records_per_combination']:.1f}

## 2. æŸ¥è¯¢æ€§èƒ½åˆ†æ

### æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ
"""

        # æŸ¥è¯¢æ€§èƒ½åˆ†æ
        benchmarks = results['query_benchmarks']
        basic_count = benchmarks['basic_count']
        sorted_query = benchmarks['sorted_query']

        report += f"""
- **åŸºæœ¬è®¡æ•°æŸ¥è¯¢**: {basic_count['duration_ms']}ms (ç»“æœ: {basic_count['result_count']} æ¡)
- **æ’åºé™åˆ¶æŸ¥è¯¢**: {sorted_query['duration_ms']}ms (ç»“æœ: {sorted_query['result_count']} æ¡)

### æ€§èƒ½é—®é¢˜è¯Šæ–­
"""

        # æ€§èƒ½é—®é¢˜åˆ†æ
        if sorted_query['duration_ms'] > 100:
            report += f"""
âš ï¸ **ä¸¥é‡æ€§èƒ½é—®é¢˜å‘ç°**:
- æ’åºæŸ¥è¯¢è€—æ—¶ {sorted_query['duration_ms']}msï¼Œè¿œè¶…é¢„æœŸ(<50ms)
- é—®é¢˜å¯èƒ½åŸå› ï¼š
  1. ç¼ºå°‘æœ‰æ•ˆçš„è¦†ç›–ç´¢å¼•
  2. ORDER BYæ“ä½œæˆæœ¬è¿‡é«˜
  3. æ•°æ®åˆ†å¸ƒå¯¼è‡´æŸ¥è¯¢é›†è¿‡å¤§

"""

        if basic_count['duration_ms'] > 10:
            report += f"""
âš ï¸ **åŸºç¡€æŸ¥è¯¢æ€§èƒ½é—®é¢˜**:
- åŸºæœ¬è¿‡æ»¤æŸ¥è¯¢è€—æ—¶ {basic_count['duration_ms']}msï¼Œè¶…è¿‡é¢„æœŸ(<10ms)
- å»ºè®®æ£€æŸ¥ç´¢å¼•ä½¿ç”¨æƒ…å†µ

"""

        # æ‰§è¡Œè®¡åˆ’åˆ†æ
        explain = benchmarks['explain_analysis']
        if 'Plan' in explain:
            plan = explain['Plan']
            report += f"""
### æ‰§è¡Œè®¡åˆ’åˆ†æ
- **æ‰§è¡Œæ—¶é—´**: {explain.get('Execution Time', 0):.2f}ms
- **è§„åˆ’æ—¶é—´**: {explain.get('Planning Time', 0):.2f}ms
- **ä¸»è¦æ“ä½œ**: {plan.get('Node Type', 'Unknown')}
- **æ‰«ææ–¹å¼**: {'ç´¢å¼•æ‰«æ' if 'Index' in plan.get('Node Type', '') else 'é¡ºåºæ‰«æ'}

"""

        # ç´¢å¼•åˆ†æ
        report += f"""
## 3. ç´¢å¼•ä½¿ç”¨åˆ†æ

### å½“å‰ç´¢å¼•çŠ¶æ€
"""
        for idx in results['index_analysis']['definitions']:
            usage_info = next((u for u in results['index_analysis']['usage_stats'] if u['indexname'] == idx['indexname']), {})
            scans = usage_info.get('idx_scan', 0)
            report += f"""
- **{idx['indexname']}**: æ‰«ææ¬¡æ•° {scans}
  ```sql
  {idx['definition']}
  ```

"""

        # å­˜å‚¨åˆ†æ
        report += f"""
## 4. å­˜å‚¨ä½¿ç”¨åˆ†æ

"""
        for table in results['storage_analysis']['table_sizes']:
            report += f"""
### {table['tablename']} è¡¨
- **æ€»å¤§å°**: {table['total_size']}
- **è¡¨å¤§å°**: {table['table_size']}
- **ç´¢å¼•å¤§å°**: {table['index_size']}

"""

        # ä¼˜åŒ–å»ºè®®
        report += f"""
## 5. ä¼˜åŒ–å»ºè®®

### ç«‹å³æ‰§è¡Œï¼ˆé«˜ä¼˜å…ˆçº§ï¼‰

1. **åˆ›å»ºè¦†ç›–ç´¢å¼•**ï¼š
   ```sql
   CREATE INDEX CONCURRENTLY idx_blue_lines_covering
   ON blue_lines (tax_rate, buyer_id, seller_id, remaining, line_id)
   WHERE remaining > 0;
   ```

2. **ä¼˜åŒ–æŸ¥è¯¢ç­–ç•¥**ï¼š
   - è€ƒè™‘ç§»é™¤ä¸å¿…è¦çš„ORDER BYæ“ä½œ
   - å®ç°æ‰¹é‡æŸ¥è¯¢å‡å°‘æ•°æ®åº“å¾€è¿”

3. **è¿æ¥æ± ä¼˜åŒ–**ï¼š
   - æ£€æŸ¥è¿æ¥é‡Šæ”¾é€»è¾‘
   - è°ƒæ•´è¿æ¥æ± é…ç½®å‚æ•°

### ä¸­æœŸæ”¹è¿›ï¼ˆä¸­ä¼˜å…ˆçº§ï¼‰

1. **éƒ¨åˆ†ç´¢å¼•ä¼˜åŒ–**ï¼š
   ```sql
   CREATE INDEX CONCURRENTLY idx_blue_lines_available_sorted
   ON blue_lines (tax_rate, buyer_id, seller_id, remaining)
   WHERE remaining > 0
   INCLUDE (line_id);
   ```

2. **æŸ¥è¯¢ç­–ç•¥æ”¹è¿›**ï¼š
   - å®ç°é¢„ç­›é€‰æœºåˆ¶
   - ä¼˜åŒ–å€™é€‰é›†å¤§å°æ§åˆ¶

### é•¿æœŸè§„åˆ’ï¼ˆä½ä¼˜å…ˆçº§ï¼‰

1. **æ•°æ®åˆ†åŒº**ï¼šè€ƒè™‘æŒ‰tax_rateæˆ–buyer_idè¿›è¡Œåˆ†åŒº
2. **ç¼“å­˜å±‚**ï¼šå®ç°å€™é€‰é›†ç¼“å­˜
3. **ç›‘æ§ä½“ç³»**ï¼šéƒ¨ç½²å®æ—¶æ€§èƒ½ç›‘æ§

## 6. é¢„æœŸæ”¹è¿›æ•ˆæœ

å®æ–½ä¸Šè¿°ä¼˜åŒ–åï¼Œé¢„æœŸæ€§èƒ½æ”¹å–„ï¼š
- **æŸ¥è¯¢æ—¶é—´**ï¼šä»{sorted_query['duration_ms']}msé™è‡³10-50ms
- **ç³»ç»Ÿååé‡**ï¼šæå‡10-50å€
- **èµ„æºä½¿ç”¨**ï¼šå‡å°‘50-80%

---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {timestamp}*
"""

        return report

    def save_results(self, results: Dict[str, Any], filename: str = None):
        """ä¿å­˜è¯Šæ–­ç»“æœåˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f"docs/performance_diagnosis_{timestamp}.md"

        # ä¿å­˜æŠ¥å‘Š
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(results['report'])

        # ä¿å­˜åŸå§‹æ•°æ®
        json_filename = filename.replace('.md', '_raw_data.json')
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(results['raw_data'], f, indent=2, ensure_ascii=False, default=str)

        print(f"âœ“ è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜: {filename}")
        print(f"âœ“ åŸå§‹æ•°æ®å·²ä¿å­˜: {json_filename}")

        return filename

    def close(self):
        """å…³é—­èµ„æº"""
        # DatabaseManager ä¼šè‡ªåŠ¨ç®¡ç†è¿æ¥æ± 


def main():
    """ä¸»å‡½æ•°"""
    print("è´Ÿæ•°å‘ç¥¨åŒ¹é…ç³»ç»Ÿ - æ•°æ®åº“æ€§èƒ½è¯Šæ–­å·¥å…·")
    print("=" * 50)

    # è·å–æ•°æ®åº“é…ç½®
    db_config = get_db_config('test')

    # åˆ›å»ºè¯Šæ–­å·¥å…·
    diagnostics = PerformanceDiagnostics(db_config)

    try:
        # è¿è¡Œè¯Šæ–­
        results = diagnostics.run_full_diagnosis()

        # ä¿å­˜ç»“æœ
        report_file = diagnostics.save_results(results)

        print(f"\nğŸ‰ æ€§èƒ½è¯Šæ–­å®Œæˆï¼")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")

        # è¾“å‡ºå…³é”®å‘ç°
        data_stats = results['raw_data']['data_stats']
        query_benchmarks = results['raw_data']['query_benchmarks']

        print(f"\nğŸ“Š å…³é”®å‘ç°:")
        print(f"  æ•°æ®é‡: {data_stats['blue_lines']['total_count']:,} æ¡")
        print(f"  å¯ç”¨æ•°æ®: {data_stats['blue_lines']['available_count']:,} æ¡ ({data_stats['blue_lines']['available_count'] / data_stats['blue_lines']['total_count'] * 100:.1f}%)")
        print(f"  æ’åºæŸ¥è¯¢è€—æ—¶: {query_benchmarks['sorted_query']['duration_ms']}ms")

        if query_benchmarks['sorted_query']['duration_ms'] > 100:
            print(f"  âš ï¸  æŸ¥è¯¢æ€§èƒ½éœ€è¦ä¼˜åŒ–")
        else:
            print(f"  âœ… æŸ¥è¯¢æ€§èƒ½è‰¯å¥½")

    except Exception as e:
        print(f"âŒ è¯Šæ–­è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise
    finally:
        diagnostics.close()


if __name__ == "__main__":
    main()