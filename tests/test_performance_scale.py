#!/usr/bin/env python3
"""
å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•ç›®çš„ï¼š
1. éªŒè¯P99å»¶è¿Ÿ<70msçš„æ€§èƒ½ç›®æ ‡
2. æµ‹è¯•åƒä¸‡çº§æ•°æ®ä¸‹çš„ç³»ç»Ÿè¡¨ç°
3. éªŒè¯ç´¢å¼•æ•ˆç‡å’ŒæŸ¥è¯¢æ€§èƒ½
4. å†…å­˜ä½¿ç”¨ç›‘æ§
5. ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

æµ‹è¯•ç¯å¢ƒï¼š
- æ•°æ®åº“ï¼šPostgreSQL 17.6
- ç¡¬ä»¶ï¼š4æ ¸8çº¿ç¨‹ï¼Œ16GBå†…å­˜ï¼Œ300GB SSD
- æµ‹è¯•æ•°æ®ï¼š100ä¸‡ - 1000ä¸‡è“ç¥¨è¡Œ

è¿è¡Œæ–¹å¼ï¼š
python test_performance_scale.py --scale small    # 100ä¸‡æ•°æ®
python test_performance_scale.py --scale medium   # 500ä¸‡æ•°æ®
python test_performance_scale.py --scale large    # 1000ä¸‡æ•°æ®
python test_performance_scale.py --scale all      # å…¨éƒ¨æµ‹è¯•
"""

import sys
import os
import argparse
import time
import psutil
import json
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from decimal import Decimal
from dataclasses import dataclass, asdict
import statistics

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.matching_engine import GreedyMatchingEngine, NegativeInvoice
from core.db_manager import DatabaseManager, CandidateProvider
from core.monitoring import get_monitor
from config.config import get_db_config
from tests.test_data_generator import TestDataGenerator


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    test_name: str
    data_scale: str
    blue_lines_count: int
    negative_invoices_count: int

    # å“åº”æ—¶é—´æŒ‡æ ‡ (æ¯«ç§’)
    response_times: List[float]
    p50_response_time: float
    p90_response_time: float
    p95_response_time: float
    p99_response_time: float
    avg_response_time: float
    max_response_time: float
    min_response_time: float

    # åŒ¹é…ç»“æœæŒ‡æ ‡
    success_rate: float
    total_matched_amount: float
    fragments_created: int

    # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
    peak_memory_mb: float
    avg_cpu_percent: float
    database_query_count: int
    database_query_time_ms: float

    # æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡
    index_scan_time_ms: float
    transaction_commit_time_ms: float

    # æµ‹è¯•é…ç½®
    batch_size: int
    sort_strategy: str
    enable_monitoring: bool

    # æ—¶é—´æˆ³
    test_timestamp: str
    duration_seconds: float


class PerformanceTestSuite:
    """å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    def __init__(self, db_config: Dict, test_config: Optional[Dict] = None):
        """
        åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•å¥—ä»¶

        Args:
            db_config: æ•°æ®åº“é…ç½®
            test_config: æµ‹è¯•é…ç½®
        """
        self.db_config = db_config
        self.db_manager = DatabaseManager(db_config)
        self.engine = GreedyMatchingEngine()
        self.candidate_provider = CandidateProvider(self.db_manager)

        # æµ‹è¯•é…ç½®
        self.test_config = test_config or {}
        self.batch_id_prefix = "perf_test"

        # æ€§èƒ½ç›‘æ§
        self.process = psutil.Process()
        self.test_results: List[PerformanceMetrics] = []

        # æ•°æ®ç”Ÿæˆå™¨
        self.data_generator = TestDataGenerator(db_config)

        # æµ‹è¯•è§„æ¨¡é…ç½®
        self.scale_configs = {
            'small': {
                'blue_lines': 1_000_000,      # 100ä¸‡
                'negative_batches': [100, 500, 1000],
                'description': 'å°è§„æ¨¡æµ‹è¯•ï¼ˆ100ä¸‡è“ç¥¨è¡Œï¼‰'
            },
            'medium': {
                'blue_lines': 5_000_000,      # 500ä¸‡
                'negative_batches': [500, 1000, 2000],
                'description': 'ä¸­ç­‰è§„æ¨¡æµ‹è¯•ï¼ˆ500ä¸‡è“ç¥¨è¡Œï¼‰'
            },
            'large': {
                'blue_lines': 10_000_000,     # 1000ä¸‡
                'negative_batches': [1000, 5000, 10000],
                'description': 'å¤§è§„æ¨¡æµ‹è¯•ï¼ˆ1000ä¸‡è“ç¥¨è¡Œï¼‰'
            }
        }

    def setup_test_data(self, scale: str) -> str:
        """
        è®¾ç½®æµ‹è¯•æ•°æ®

        Args:
            scale: æµ‹è¯•è§„æ¨¡ (small/medium/large)

        Returns:
            str: æ‰¹æ¬¡ID
        """
        config = self.scale_configs.get(scale)
        if not config:
            raise ValueError(f"ä¸æ”¯æŒçš„æµ‹è¯•è§„æ¨¡: {scale}")

        blue_lines_count = config['blue_lines']
        batch_id = f"{self.batch_id_prefix}_{scale}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        print(f"\n=== è®¾ç½®{config['description']} ===")
        print(f"å‡†å¤‡ç”Ÿæˆ {blue_lines_count:,} æ¡è“ç¥¨è¡Œæ•°æ®...")

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒè§„æ¨¡çš„æ•°æ®
        existing_batch = self._find_existing_batch(blue_lines_count)
        if existing_batch:
            print(f"å‘ç°å·²å­˜åœ¨çš„ç›¸åŒè§„æ¨¡æ•°æ®æ‰¹æ¬¡: {existing_batch}")
            choice = input("æ˜¯å¦é‡ç”¨ç°æœ‰æ•°æ®ï¼Ÿ(y/n): ").lower()
            if choice == 'y':
                return existing_batch

        # ç”Ÿæˆæ–°æ•°æ®
        start_time = time.time()
        actual_batch_id = self.data_generator.generate_blue_lines(
            total_lines=blue_lines_count,
            batch_id=batch_id
        )
        generation_time = time.time() - start_time

        print(f"âœ“ æ•°æ®ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time:.2f}ç§’")
        print(f"âœ“ æ‰¹æ¬¡ID: {actual_batch_id}")

        return actual_batch_id

    def _find_existing_batch(self, target_lines: int) -> Optional[str]:
        """æŸ¥æ‰¾å·²å­˜åœ¨çš„ç›¸åŒè§„æ¨¡æ‰¹æ¬¡"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT batch_id FROM batch_metadata
                    WHERE total_lines = %s AND status = 'completed'
                    ORDER BY end_time DESC LIMIT 1
                """, (target_lines,))
                result = cur.fetchone()
                return result[0] if result else None
        finally:
            self.db_manager.pool.putconn(conn)

    def run_performance_test(self, scale: str, batch_id: str) -> List[PerformanceMetrics]:
        """
        è¿è¡Œæ€§èƒ½æµ‹è¯•

        Args:
            scale: æµ‹è¯•è§„æ¨¡
            batch_id: æ•°æ®æ‰¹æ¬¡ID

        Returns:
            List[PerformanceMetrics]: æµ‹è¯•ç»“æœ
        """
        config = self.scale_configs[scale]
        negative_batches = config['negative_batches']

        print(f"\n=== æ‰§è¡Œ{config['description']}æ€§èƒ½æµ‹è¯• ===")

        scale_results = []

        for negative_count in negative_batches:
            print(f"\n--- æµ‹è¯•æ‰¹æ¬¡: {negative_count} ä¸ªè´Ÿæ•°å‘ç¥¨ ---")

            # ç”Ÿæˆè´Ÿæ•°å‘ç¥¨
            negatives = self.data_generator.generate_negative_invoices_objects(
                scenario="mixed",
                count=negative_count
            )

            # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
            metrics = self._execute_single_test(
                test_name=f"{scale}_{negative_count}",
                scale=scale,
                negatives=negatives,
                batch_id=batch_id
            )

            scale_results.append(metrics)

            # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
            self._print_test_summary(metrics)

            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…ç³»ç»Ÿè¿‡çƒ­
            time.sleep(2)

        return scale_results

    def _execute_single_test(self, test_name: str, scale: str,
                           negatives: List[NegativeInvoice], batch_id: str) -> PerformanceMetrics:
        """
        æ‰§è¡Œå•ä¸ªæ€§èƒ½æµ‹è¯•

        Args:
            test_name: æµ‹è¯•åç§°
            scale: æµ‹è¯•è§„æ¨¡
            negatives: è´Ÿæ•°å‘ç¥¨åˆ—è¡¨
            batch_id: æ•°æ®æ‰¹æ¬¡ID

        Returns:
            PerformanceMetrics: æ€§èƒ½æŒ‡æ ‡
        """
        # é‡ç½®ç›‘æ§çŠ¶æ€
        monitor = get_monitor()
        monitor.reset_stats()

        # é¢„çƒ­ï¼šæ‰§è¡Œä¸€æ¬¡å°è§„æ¨¡æŸ¥è¯¢
        warmup_negatives = negatives[:5] if len(negatives) > 5 else negatives
        self.engine.match_batch(warmup_negatives, self.candidate_provider)
        time.sleep(1)  # ç­‰å¾…é¢„çƒ­å®Œæˆ

        # å¼€å§‹æ€§èƒ½ç›‘æ§
        start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        start_time = time.time()
        cpu_samples = []
        memory_samples = []

        # è®°å½•æ•°æ®åº“æŸ¥è¯¢å‰çŠ¶æ€
        db_query_start = time.time()

        # æ‰§è¡ŒåŒ¹é…æµ‹è¯•
        response_times = []

        # å°†è´Ÿæ•°å‘ç¥¨åˆ†æˆå¤šä¸ªå°æ‰¹æ¬¡ä»¥è·å¾—æ›´å¤šå“åº”æ—¶é—´æ ·æœ¬
        batch_size = min(100, len(negatives))  # æ¯æ‰¹æœ€å¤š100ä¸ª
        all_results = []

        for i in range(0, len(negatives), batch_size):
            batch_negatives = negatives[i:i + batch_size]

            # è®°å½•å•æ‰¹æ¬¡æ€§èƒ½
            batch_start = time.time()

            # ç›‘æ§ç³»ç»Ÿèµ„æº
            cpu_samples.append(psutil.cpu_percent())
            memory_samples.append(self.process.memory_info().rss / 1024 / 1024)

            # æ‰§è¡ŒåŒ¹é…
            batch_results = self.engine.match_batch(
                batch_negatives,
                self.candidate_provider,
                sort_strategy="amount_desc",
                enable_monitoring=True
            )

            batch_time = (time.time() - batch_start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            response_times.append(batch_time)
            all_results.extend(batch_results)

            # æ¯æ‰¹æ¬¡ä¹‹é—´çŸ­æš‚ä¼‘æ¯
            time.sleep(0.1)

        # æµ‹è¯•ç»“æŸ
        total_duration = time.time() - start_time
        db_query_time = (time.time() - db_query_start) * 1000  # æ¯«ç§’
        peak_memory = max(memory_samples) if memory_samples else start_memory
        avg_cpu = statistics.mean(cpu_samples) if cpu_samples else 0

        # è®¡ç®—å“åº”æ—¶é—´ç»Ÿè®¡
        if response_times:
            p50 = statistics.median(response_times)
            p90 = self._percentile(response_times, 90)
            p95 = self._percentile(response_times, 95)
            p99 = self._percentile(response_times, 99)
            avg_time = statistics.mean(response_times)
            max_time = max(response_times)
            min_time = min(response_times)
        else:
            p50 = p90 = p95 = p99 = avg_time = max_time = min_time = 0

        # è®¡ç®—åŒ¹é…ç»“æœç»Ÿè®¡
        success_count = sum(1 for r in all_results if r.success)
        success_rate = success_count / len(all_results) if all_results else 0
        total_matched = sum(r.total_matched for r in all_results)
        fragments = sum(r.fragments_created for r in all_results)

        # è·å–è“ç¥¨è¡Œæ•°é‡ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        blue_lines_count = self._get_blue_lines_count(batch_id)

        # è·å–ç›‘æ§æ•°æ®
        health_report = monitor.get_health_report()
        technical_metrics = health_report.get('technical_metrics', {})
        performance_metrics = health_report.get('performance_metrics', {})

        # æ„é€ æ€§èƒ½æŒ‡æ ‡å¯¹è±¡
        metrics = PerformanceMetrics(
            test_name=test_name,
            data_scale=scale,
            blue_lines_count=blue_lines_count,
            negative_invoices_count=len(negatives),

            # å“åº”æ—¶é—´æŒ‡æ ‡
            response_times=response_times,
            p50_response_time=p50,
            p90_response_time=p90,
            p95_response_time=p95,
            p99_response_time=p99,
            avg_response_time=avg_time,
            max_response_time=max_time,
            min_response_time=min_time,

            # åŒ¹é…ç»“æœæŒ‡æ ‡
            success_rate=success_rate,
            total_matched_amount=float(total_matched),
            fragments_created=fragments,

            # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
            peak_memory_mb=peak_memory,
            avg_cpu_percent=avg_cpu,
            database_query_count=performance_metrics.get('total_requests', 0),
            database_query_time_ms=db_query_time,

            # æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡ï¼ˆç®€åŒ–ï¼‰
            index_scan_time_ms=db_query_time * 0.4,  # ä¼°ç®—40%æ—¶é—´ç”¨äºç´¢å¼•æ‰«æ
            transaction_commit_time_ms=db_query_time * 0.2,  # ä¼°ç®—20%æ—¶é—´ç”¨äºäº‹åŠ¡æäº¤

            # æµ‹è¯•é…ç½®
            batch_size=batch_size,
            sort_strategy="amount_desc",
            enable_monitoring=True,

            # æ—¶é—´æˆ³
            test_timestamp=datetime.now().isoformat(),
            duration_seconds=total_duration
        )

        return metrics

    def _percentile(self, data: List[float], percentile: float) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = (percentile / 100) * (len(sorted_data) - 1)
        if index.is_integer():
            return sorted_data[int(index)]
        else:
            lower_index = int(index)
            upper_index = lower_index + 1
            weight = index - lower_index
            return sorted_data[lower_index] * (1 - weight) + sorted_data[upper_index] * weight

    def _get_blue_lines_count(self, batch_id: str) -> int:
        """è·å–æŒ‡å®šæ‰¹æ¬¡çš„è“ç¥¨è¡Œæ•°é‡"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT COUNT(*) FROM blue_lines WHERE batch_id = %s", (batch_id,))
                return cur.fetchone()[0]
        finally:
            self.db_manager.pool.putconn(conn)

    def _print_test_summary(self, metrics: PerformanceMetrics):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print(f"  âœ“ æµ‹è¯•å®Œæˆ: {metrics.test_name}")
        print(f"    æ•°æ®è§„æ¨¡: {metrics.blue_lines_count:,} è“ç¥¨è¡Œ, {metrics.negative_invoices_count} è´Ÿæ•°å‘ç¥¨")
        print(f"    åŒ¹é…æˆåŠŸç‡: {metrics.success_rate:.1%}")
        print(f"    å“åº”æ—¶é—´: P50={metrics.p50_response_time:.1f}ms, P99={metrics.p99_response_time:.1f}ms")
        print(f"    å†…å­˜å³°å€¼: {metrics.peak_memory_mb:.1f}MB")
        print(f"    æ€»è€—æ—¶: {metrics.duration_seconds:.2f}ç§’")

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€§èƒ½ç›®æ ‡
        if metrics.p99_response_time <= 70:
            print(f"    ğŸ¯ P99æ€§èƒ½ç›®æ ‡è¾¾æˆ: {metrics.p99_response_time:.1f}ms â‰¤ 70ms")
        else:
            print(f"    âš ï¸  P99æ€§èƒ½ç›®æ ‡æœªè¾¾æˆ: {metrics.p99_response_time:.1f}ms > 70ms")

        if metrics.success_rate >= 0.93:
            print(f"    ğŸ¯ åŒ¹é…ç‡ç›®æ ‡è¾¾æˆ: {metrics.success_rate:.1%} â‰¥ 93%")
        else:
            print(f"    âš ï¸  åŒ¹é…ç‡ç›®æ ‡æœªè¾¾æˆ: {metrics.success_rate:.1%} < 93%")

    def generate_performance_report(self, results: List[PerformanceMetrics],
                                  output_file: Optional[str] = None) -> str:
        """
        ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š

        Args:
            results: æµ‹è¯•ç»“æœåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        if not results:
            return "æ— æµ‹è¯•ç»“æœ"

        # è·å–ç³»ç»Ÿä¿¡æ¯
        system_info = {
            'cpu_count': psutil.cpu_count(),
            'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
            'memory_total_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
            'python_version': sys.version,
            'postgresql_version': self._get_postgresql_version()
        }

        # ç”ŸæˆæŠ¥å‘Š
        report = self._format_performance_report(results, system_info)

        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"âœ“ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")

        return report

    def _get_postgresql_version(self) -> str:
        """è·å–PostgreSQLç‰ˆæœ¬"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT version()")
                return cur.fetchone()[0]
        except:
            return "Unknown"
        finally:
            self.db_manager.pool.putconn(conn)

    def _format_performance_report(self, results: List[PerformanceMetrics],
                                 system_info: Dict) -> str:
        """æ ¼å¼åŒ–æ€§èƒ½æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        report = f"""
# è´Ÿæ•°å‘ç¥¨åŒ¹é…ç³»ç»Ÿ - å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚å†µ
- **æµ‹è¯•æ—¶é—´**: {timestamp}
- **æµ‹è¯•ç›®æ ‡**: éªŒè¯P99å»¶è¿Ÿ<70msï¼ŒåŒ¹é…ç‡>93%
- **æµ‹è¯•ç”¨ä¾‹æ•°**: {len(results)}

## æµ‹è¯•ç¯å¢ƒ

### ç¡¬ä»¶é…ç½®
- **CPU**: {system_info['cpu_count']} æ ¸å¿ƒ
- **å†…å­˜**: {system_info['memory_total_gb']:.1f} GB
- **å­˜å‚¨**: SSD (300GB)

### è½¯ä»¶é…ç½®
- **æ•°æ®åº“**: PostgreSQL 17.6
- **Python**: {system_info['python_version'].split()[0]}
- **æ•°æ®åº“ç‰ˆæœ¬**: {system_info['postgresql_version']}

## æµ‹è¯•ç»“æœæ±‡æ€»

### å…³é”®æŒ‡æ ‡è¾¾æˆæƒ…å†µ
"""

        # æ£€æŸ¥å…³é”®æŒ‡æ ‡è¾¾æˆæƒ…å†µ
        p99_passed = sum(1 for r in results if r.p99_response_time <= 70)
        success_rate_passed = sum(1 for r in results if r.success_rate >= 0.93)

        report += f"""
| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¾¾æˆç‡ | çŠ¶æ€ |
|------|--------|--------|------|
| P99å“åº”æ—¶é—´ | â‰¤70ms | {p99_passed}/{len(results)} ({p99_passed/len(results):.1%}) | {'âœ…' if p99_passed == len(results) else 'âš ï¸'} |
| åŒ¹é…æˆåŠŸç‡ | â‰¥93% | {success_rate_passed}/{len(results)} ({success_rate_passed/len(results):.1%}) | {'âœ…' if success_rate_passed == len(results) else 'âš ï¸'} |

### è¯¦ç»†æµ‹è¯•ç»“æœ

"""

        # è¯¦ç»†ç»“æœè¡¨æ ¼
        report += "| æµ‹è¯•è§„æ¨¡ | è“ç¥¨è¡Œæ•° | è´Ÿæ•°å‘ç¥¨æ•° | P50(ms) | P90(ms) | P95(ms) | P99(ms) | åŒ¹é…ç‡ | å†…å­˜å³°å€¼(MB) | æ€»è€—æ—¶(s) |\n"
        report += "|----------|----------|------------|---------|---------|---------|---------|---------|-------------|----------|\n"

        for r in results:
            report += f"| {r.data_scale} | {r.blue_lines_count:,} | {r.negative_invoices_count} | "
            report += f"{r.p50_response_time:.1f} | {r.p90_response_time:.1f} | {r.p95_response_time:.1f} | "
            report += f"{r.p99_response_time:.1f} | {r.success_rate:.1%} | {r.peak_memory_mb:.1f} | {r.duration_seconds:.2f} |\n"

        # æ€§èƒ½åˆ†æ
        report += "\n## æ€§èƒ½åˆ†æ\n\n"

        # æœ€ä½³å’Œæœ€å·®æ€§èƒ½
        best_p99 = min(results, key=lambda x: x.p99_response_time)
        worst_p99 = max(results, key=lambda x: x.p99_response_time)
        best_success = max(results, key=lambda x: x.success_rate)
        worst_success = min(results, key=lambda x: x.success_rate)

        report += f"""### æ€§èƒ½è¡¨ç°
- **æœ€ä½³P99å“åº”æ—¶é—´**: {best_p99.p99_response_time:.1f}ms ({best_p99.test_name})
- **æœ€å·®P99å“åº”æ—¶é—´**: {worst_p99.p99_response_time:.1f}ms ({worst_p99.test_name})
- **æœ€é«˜åŒ¹é…æˆåŠŸç‡**: {best_success.success_rate:.1%} ({best_success.test_name})
- **æœ€ä½åŒ¹é…æˆåŠŸç‡**: {worst_success.success_rate:.1%} ({worst_success.test_name})

### èµ„æºä½¿ç”¨åˆ†æ
"""

        max_memory = max(r.peak_memory_mb for r in results)
        avg_memory = sum(r.peak_memory_mb for r in results) / len(results)
        max_cpu = max(r.avg_cpu_percent for r in results)

        report += f"""- **å†…å­˜å³°å€¼**: {max_memory:.1f}MB
- **å¹³å‡å†…å­˜ä½¿ç”¨**: {avg_memory:.1f}MB
- **CPUå³°å€¼**: {max_cpu:.1f}%

### å¯æ‰©å±•æ€§åˆ†æ
"""

        # æŒ‰è§„æ¨¡åˆ†ç»„åˆ†æ
        scale_groups = {}
        for r in results:
            if r.data_scale not in scale_groups:
                scale_groups[r.data_scale] = []
            scale_groups[r.data_scale].append(r)

        for scale, scale_results in scale_groups.items():
            avg_p99 = sum(r.p99_response_time for r in scale_results) / len(scale_results)
            avg_success = sum(r.success_rate for r in scale_results) / len(scale_results)
            data_size = scale_results[0].blue_lines_count

            report += f"- **{scale}è§„æ¨¡** ({data_size:,}æ¡æ•°æ®): å¹³å‡P99={avg_p99:.1f}ms, å¹³å‡åŒ¹é…ç‡={avg_success:.1%}\n"

        # ç»“è®ºå’Œå»ºè®®
        report += "\n## ç»“è®ºä¸å»ºè®®\n\n"

        overall_p99_pass = p99_passed == len(results)
        overall_success_pass = success_rate_passed == len(results)

        if overall_p99_pass and overall_success_pass:
            report += "âœ… **æ€»ä½“è¯„ä¼°**: ç³»ç»Ÿæ€§èƒ½å®Œå…¨æ»¡è¶³è®¾è®¡ç›®æ ‡ï¼Œå¯ä»¥æ”¯æ’‘ç”Ÿäº§ç¯å¢ƒè¿è¡Œã€‚\n\n"
        elif overall_p99_pass or overall_success_pass:
            report += "âš ï¸ **æ€»ä½“è¯„ä¼°**: ç³»ç»Ÿæ€§èƒ½éƒ¨åˆ†æ»¡è¶³è®¾è®¡ç›®æ ‡ï¼Œå»ºè®®é’ˆå¯¹æ€§ä¼˜åŒ–ã€‚\n\n"
        else:
            report += "âŒ **æ€»ä½“è¯„ä¼°**: ç³»ç»Ÿæ€§èƒ½æœªè¾¾åˆ°è®¾è®¡ç›®æ ‡ï¼Œéœ€è¦é‡å¤§ä¼˜åŒ–ã€‚\n\n"

        # å…·ä½“å»ºè®®
        report += "### ä¼˜åŒ–å»ºè®®\n"

        if not overall_p99_pass:
            report += "- **æ€§èƒ½ä¼˜åŒ–**: P99å“åº”æ—¶é—´è¶…æ ‡ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œç´¢å¼•ç­–ç•¥\n"

        if not overall_success_pass:
            report += "- **ç®—æ³•ä¼˜åŒ–**: åŒ¹é…æˆåŠŸç‡ä¸è¾¾æ ‡ï¼Œå»ºè®®è°ƒæ•´è´ªå©ªç®—æ³•å‚æ•°æˆ–å€™é€‰é›†å¤§å°\n"

        if max_memory > 2000:  # 2GB
            report += "- **å†…å­˜ä¼˜åŒ–**: å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä½¿ç”¨æµå¼å¤„ç†å‡å°‘å†…å­˜å ç”¨\n"

        report += "- **ç›‘æ§å»ºè®®**: å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²å®æ—¶æ€§èƒ½ç›‘æ§\n"
        report += "- **å®¹é‡è§„åˆ’**: åŸºäºæµ‹è¯•ç»“æœåˆ¶å®šåˆç†çš„å®¹é‡è§„åˆ’ç­–ç•¥\n"

        # JSONæ•°æ®ï¼ˆç”¨äºè¿›ä¸€æ­¥åˆ†æï¼‰
        report += "\n## åŸå§‹æ•°æ®\n\n"
        report += "```json\n"
        json_data = {
            'system_info': system_info,
            'test_results': [asdict(r) for r in results],
            'summary': {
                'total_tests': len(results),
                'p99_target_achieved': p99_passed,
                'success_rate_target_achieved': success_rate_passed,
                'best_p99_ms': best_p99.p99_response_time,
                'worst_p99_ms': worst_p99.p99_response_time,
                'best_success_rate': best_success.success_rate,
                'worst_success_rate': worst_success.success_rate
            }
        }
        report += json.dumps(json_data, indent=2, ensure_ascii=False)
        report += "\n```\n"

        return report

    def cleanup_test_data(self, batch_id: str):
        """æ¸…ç†æµ‹è¯•æ•°æ®"""
        print(f"\næ¸…ç†æµ‹è¯•æ•°æ®: {batch_id}")
        self.data_generator.clear_batch(batch_id)

    def close(self):
        """å…³é—­èµ„æº"""
        self.data_generator.close()


def run_performance_tests(scales: List[str], cleanup: bool = True,
                         report_file: Optional[str] = None):
    """
    è¿è¡Œæ€§èƒ½æµ‹è¯•

    Args:
        scales: æµ‹è¯•è§„æ¨¡åˆ—è¡¨
        cleanup: æ˜¯å¦æ¸…ç†æµ‹è¯•æ•°æ®
        report_file: æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶
    """
    print("=== è´Ÿæ•°å‘ç¥¨åŒ¹é…ç³»ç»Ÿ - å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯• ===\n")

    # åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶
    db_config = get_db_config('test')
    test_suite = PerformanceTestSuite(db_config)

    all_results = []
    batch_ids = []

    try:
        for scale in scales:
            print(f"\n{'='*60}")
            print(f"å¼€å§‹ {scale} è§„æ¨¡æµ‹è¯•")
            print(f"{'='*60}")

            # è®¾ç½®æµ‹è¯•æ•°æ®
            batch_id = test_suite.setup_test_data(scale)
            batch_ids.append(batch_id)

            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            results = test_suite.run_performance_test(scale, batch_id)
            all_results.extend(results)

            print(f"\nâœ“ {scale} è§„æ¨¡æµ‹è¯•å®Œæˆ")

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        print(f"\n{'='*60}")
        print("ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print(f"{'='*60}")

        if not report_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = f"docs/performance_test_report_{timestamp}.md"

        report = test_suite.generate_performance_report(all_results, report_file)

        # è¾“å‡ºç®€è¦ç»“æœ
        print("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
        p99_passed = sum(1 for r in all_results if r.p99_response_time <= 70)
        success_rate_passed = sum(1 for r in all_results if r.success_rate >= 0.93)

        print(f"  æ€»æµ‹è¯•ç”¨ä¾‹: {len(all_results)}")
        print(f"  P99ç›®æ ‡è¾¾æˆ: {p99_passed}/{len(all_results)} ({p99_passed/len(all_results):.1%})")
        print(f"  åŒ¹é…ç‡ç›®æ ‡è¾¾æˆ: {success_rate_passed}/{len(all_results)} ({success_rate_passed/len(all_results):.1%})")

        if p99_passed == len(all_results) and success_rate_passed == len(all_results):
            print("  ğŸ‰ æ‰€æœ‰æ€§èƒ½ç›®æ ‡è¾¾æˆï¼")
        else:
            print("  âš ï¸  éƒ¨åˆ†æ€§èƒ½ç›®æ ‡æœªè¾¾æˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise
    finally:
        # æ¸…ç†æµ‹è¯•æ•°æ®
        if cleanup:
            print(f"\nğŸ§¹ æ¸…ç†æµ‹è¯•æ•°æ®...")
            for batch_id in batch_ids:
                try:
                    test_suite.cleanup_test_data(batch_id)
                except Exception as e:
                    print(f"âš ï¸ æ¸…ç†æ‰¹æ¬¡ {batch_id} å¤±è´¥: {e}")

        test_suite.close()


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='è´Ÿæ•°å‘ç¥¨åŒ¹é…ç³»ç»Ÿå¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å°è§„æ¨¡æµ‹è¯•
  python test_performance_scale.py --scale small

  # æ‰€æœ‰è§„æ¨¡æµ‹è¯•
  python test_performance_scale.py --scale all

  # è‡ªå®šä¹‰æŠ¥å‘Šæ–‡ä»¶å
  python test_performance_scale.py --scale medium --report performance_2025.md

  # ä¿ç•™æµ‹è¯•æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
  python test_performance_scale.py --scale small --no-cleanup
        """
    )

    parser.add_argument('--scale',
                       choices=['small', 'medium', 'large', 'all'],
                       default='small',
                       help='æµ‹è¯•è§„æ¨¡ (é»˜è®¤: small)')

    parser.add_argument('--report', type=str,
                       help='æ€§èƒ½æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (é»˜è®¤: docs/performance_test_report_TIMESTAMP.md)')

    parser.add_argument('--no-cleanup', action='store_true',
                       help='ä¸æ¸…ç†æµ‹è¯•æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰')

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    # ç¡®å®šæµ‹è¯•è§„æ¨¡
    if args.scale == 'all':
        scales = ['small', 'medium', 'large']
    else:
        scales = [args.scale]

    # è¿è¡Œæµ‹è¯•
    run_performance_tests(
        scales=scales,
        cleanup=not args.no_cleanup,
        report_file=args.report
    )