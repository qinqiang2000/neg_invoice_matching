#!/usr/bin/env python3
"""
å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•è„šæœ¬

æµ‹è¯•ç›®çš„ï¼š
1. éªŒè¯P99å»¶è¿Ÿ<70msçš„æ€§èƒ½ç›®æ ‡
2. æµ‹è¯•åƒä¸‡çº§æ•°æ®ä¸‹çš„ç³»ç»Ÿè¡¨ç°
3. éªŒè¯ç´¢å¼•æ•ˆç‡å’ŒæŸ¥è¯¢æ€§èƒ½
4. å†…å­˜ä½¿ç”¨ç›‘æ§
5. ç”Ÿæˆè¯¦ç»†çš„æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

æµ‹è¯•ç¯å¢ƒï¼š
- æ•°æ®åº“ï¼šPostgreSQL 17.6
- ç¡¬ä»¶ï¼š4æ ¸8çº¿ç¨‹ï¼Œ16GBå†…å­˜ï¼Œ300GB SSD
- æµ‹è¯•æ•°æ®ï¼š100ä¸‡ - 1000ä¸‡è“ç¥¨è¡Œ

è¿è¡Œæ–¹å¼ï¼š
python test_performance_scale.py --scale small    # 100ä¸‡æ•°æ®
python test_performance_scale.py --scale medium   # 500ä¸‡æ•°æ®
python test_performance_scale.py --scale large    # 1000ä¸‡æ•°æ®
python test_performance_scale.py --scale all      # å…¨éƒ¨æµ‹è¯•
"""

import sys
import os
import argparse
import time
import psutil
import json
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from decimal import Decimal
from dataclasses import dataclass, asdict
import statistics

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from core.matching_engine import GreedyMatchingEngine, NegativeInvoice
from core.db_manager import DatabaseManager, CandidateProvider
from core.monitoring import get_monitor
from core.performance_monitor import get_performance_timer, reset_performance_timer
from core.explainability import ExplainabilityReporter
from core.diagnostics import MatchDiagnostics
from config.config import get_db_config
from tests.test_data_generator import TestDataGenerator


@dataclass
class PerformanceMetrics:
    """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç»“æ„"""
    test_name: str
    data_scale: str
    batch_blue_lines_count: int  # æµ‹è¯•æ‰¹æ¬¡çš„æ•°æ®é‡
    total_blue_lines_count: int  # æ•°æ®åº“æ€»æ•°æ®é‡
    negative_invoices_count: int

    # å“åº”æ—¶é—´æŒ‡æ ‡ (æ¯«ç§’)
    response_times: List[float]
    p50_response_time: float
    p90_response_time: float
    p95_response_time: float
    p99_response_time: float
    avg_response_time: float
    max_response_time: float
    min_response_time: float

    # å•æ¬¡æ€§èƒ½æŒ‡æ ‡ (æ¯«ç§’)
    avg_single_query_time_ms: float
    avg_single_match_time_ms: float

    # åŒ¹é…ç»“æœæŒ‡æ ‡
    success_rate: float
    total_matched_amount: float
    fragments_created: int

    # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
    peak_memory_mb: float
    avg_cpu_percent: float
    database_query_count: int
    database_query_time_ms: float

    # è¯¦ç»†æ€§èƒ½åˆ†è§£æŒ‡æ ‡
    detailed_performance_breakdown: Dict[str, float]

    # æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡
    index_scan_time_ms: float
    transaction_commit_time_ms: float

    # æµ‹è¯•é…ç½®
    batch_size: int
    sort_strategy: str
    enable_monitoring: bool

    # æ—¶é—´æˆ³
    test_timestamp: str
    duration_seconds: float


class PerformanceTestSuite:
    """å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•å¥—ä»¶"""

    def __init__(self, db_config: Dict, test_config: Optional[Dict] = None, preserve_data: bool = False,
                 enable_explainability: bool = True, enable_deep_diagnosis: bool = False,
                 seed: Optional[int] = None):
        """
        åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•å¥—ä»¶

        Args:
            db_config: æ•°æ®åº“é…ç½®
            test_config: æµ‹è¯•é…ç½®
            preserve_data: æ•°æ®ä¿ç•™ç­–ç•¥
                - False (é»˜è®¤): æµ‹è¯•åé‡ç½®æ•°æ®çŠ¶æ€ï¼Œä¿ç•™æ•°æ®è®°å½•ä¾›ä¸‹æ¬¡æµ‹è¯•å¤ç”¨
                - True: ä¿ç•™æµ‹è¯•åçš„æ•°æ®çŠ¶æ€ï¼Œç”¨äºåç»­åˆ†æ
            enable_explainability: æ˜¯å¦å¯ç”¨å¯è§£é‡Šæ€§åˆ†æï¼ˆé»˜è®¤Trueï¼Œå‡ ä¹æ— æ€§èƒ½å½±å“ï¼‰
            enable_deep_diagnosis: æ˜¯å¦å¯ç”¨æ·±åº¦è¯Šæ–­ï¼ˆé»˜è®¤Falseï¼Œå¯é€‰æ‹©æ€§å¯ç”¨ï¼‰
            seed: éšæœºç§å­ï¼ˆå¯é€‰ï¼Œç”¨äºç”Ÿæˆå¯é‡å¤çš„æµ‹è¯•æ•°æ®ï¼‰
        """
        self.db_config = db_config
        self.preserve_data = preserve_data
        self.enable_explainability = enable_explainability
        self.enable_deep_diagnosis = enable_deep_diagnosis
        self.seed = seed
        self.test_batch_ids = []  # è·Ÿè¸ªæµ‹è¯•ç”Ÿæˆçš„æ‰¹æ¬¡ID
        self.db_manager = DatabaseManager(db_config)
        self.engine = GreedyMatchingEngine(debug_mode=False)  # é»˜è®¤å…³é—­è°ƒè¯•è¾“å‡º
        self.candidate_provider = CandidateProvider(self.db_manager)

        # æµ‹è¯•é…ç½®
        self.test_config = test_config or {}
        self.batch_id_prefix = "perf_test"

        # æ€§èƒ½ç›‘æ§
        self.process = psutil.Process()
        self.test_results: List[PerformanceMetrics] = []

        # æ•°æ®ç”Ÿæˆå™¨ï¼ˆæ”¯æŒå›ºå®šç§å­ï¼‰
        self.data_generator = TestDataGenerator(db_config, seed=seed)

        # å¯è§£é‡Šæ€§åŠŸèƒ½
        if self.enable_explainability:
            self.explainability_reporter = ExplainabilityReporter(self.db_manager)
            self.all_match_results: List = []  # æ”¶é›†æ‰€æœ‰åŒ¹é…ç»“æœç”¨äºåˆ†æ
            self.all_negatives: List[NegativeInvoice] = []  # æ”¶é›†æ‰€æœ‰è´Ÿæ•°å‘ç¥¨

        if self.enable_deep_diagnosis:
            self.diagnostics = MatchDiagnostics(self.db_manager)

        # æµ‹è¯•è§„æ¨¡é…ç½®
        self.scale_configs = {
            'small': {
                'blue_lines': 1_000_000,      # 100ä¸‡
                'negative_batches': [1000],
                'description': 'å°è§„æ¨¡æµ‹è¯•ï¼ˆ100ä¸‡è“ç¥¨è¡Œï¼‰'
            },
            'medium': {
                'blue_lines': 5_000_000,      # 500ä¸‡
                'negative_batches': [2000],
                'description': 'ä¸­ç­‰è§„æ¨¡æµ‹è¯•ï¼ˆ500ä¸‡è“ç¥¨è¡Œï¼‰'
            },
            'large': {
                'blue_lines': 10_000_000,     # 1000ä¸‡
                'negative_batches': [ 3000],
                'description': 'å¤§è§„æ¨¡æµ‹è¯•ï¼ˆ1000ä¸‡è“ç¥¨è¡Œï¼‰'
            }
        }

    def setup_test_data(self, scale: str) -> str:
        """
        è®¾ç½®æµ‹è¯•æ•°æ®

        Args:
            scale: æµ‹è¯•è§„æ¨¡ (small/medium/large)

        Returns:
            str: æ‰¹æ¬¡ID
        """
        config = self.scale_configs.get(scale)
        if not config:
            raise ValueError(f"ä¸æ”¯æŒçš„æµ‹è¯•è§„æ¨¡: {scale}")

        blue_lines_count = config['blue_lines']
        batch_id = f"{self.batch_id_prefix}_{scale}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

        print(f"\n=== è®¾ç½®{config['description']} ===")
        print(f"å‡†å¤‡ç”Ÿæˆ {blue_lines_count:,} æ¡è“ç¥¨è¡Œæ•°æ®...")

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒè§„æ¨¡çš„æ•°æ®
        existing_batch = self._find_existing_batch(blue_lines_count)
        if existing_batch:
            print(f"å‘ç°å·²å­˜åœ¨çš„ç›¸åŒè§„æ¨¡æ•°æ®æ‰¹æ¬¡: {existing_batch}")
            print("è‡ªåŠ¨é‡ç”¨ç°æœ‰æ•°æ®...")
            return existing_batch

        # ç”Ÿæˆæ–°æ•°æ®
        start_time = time.time()
        actual_batch_id = self.data_generator.generate_blue_lines(
            total_lines=blue_lines_count,
            batch_id=batch_id
        )
        generation_time = time.time() - start_time

        print(f"âœ“ æ•°æ®ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶: {generation_time:.2f}ç§’")
        print(f"âœ“ æ‰¹æ¬¡ID: {actual_batch_id}")

        # è®°å½•æ–°ç”Ÿæˆçš„æ‰¹æ¬¡ID
        self.test_batch_ids.append(actual_batch_id)

        return actual_batch_id

    def _find_existing_batch(self, target_lines: int) -> Optional[str]:
        """æŸ¥æ‰¾å·²å­˜åœ¨çš„ç›¸åŒè§„æ¨¡æ‰¹æ¬¡"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT batch_id FROM batch_metadata
                    WHERE total_lines = %s AND status = 'completed'
                    ORDER BY end_time DESC LIMIT 1
                """, (target_lines,))
                result = cur.fetchone()
                return result[0] if result else None
        finally:
            self.db_manager.pool.putconn(conn)

    def run_performance_test(self, scale: str, batch_id: str) -> List[PerformanceMetrics]:
        """
        è¿è¡Œæ€§èƒ½æµ‹è¯•

        Args:
            scale: æµ‹è¯•è§„æ¨¡
            batch_id: æ•°æ®æ‰¹æ¬¡ID

        Returns:
            List[PerformanceMetrics]: æµ‹è¯•ç»“æœ
        """
        config = self.scale_configs[scale]
        negative_batches = config['negative_batches']

        print(f"\n=== æ‰§è¡Œ{config['description']}æ€§èƒ½æµ‹è¯• ===")

        scale_results = []

        for negative_count in negative_batches:
            print(f"\n--- æµ‹è¯•æ‰¹æ¬¡: {negative_count} ä¸ªè´Ÿæ•°å‘ç¥¨ ---")

            # ç”Ÿæˆè´Ÿæ•°å‘ç¥¨
            negatives = self.data_generator.generate_negative_invoices_objects(
                scenario="mixed",
                count=negative_count
            )

            # æ‰§è¡Œæ€§èƒ½æµ‹è¯•
            metrics = self._execute_single_test(
                test_name=f"{scale}_{negative_count}",
                scale=scale,
                negatives=negatives,
                batch_id=batch_id
            )

            scale_results.append(metrics)

            # è¾“å‡ºæµ‹è¯•ç»“æœæ‘˜è¦
            self._print_test_summary(metrics)

            # çŸ­æš‚ä¼‘æ¯ï¼Œé¿å…ç³»ç»Ÿè¿‡çƒ­
            time.sleep(2)

        return scale_results

    def _execute_single_test(self, test_name: str, scale: str,
                           negatives: List[NegativeInvoice], batch_id: str) -> PerformanceMetrics:
        """
        æ‰§è¡Œå•ä¸ªæ€§èƒ½æµ‹è¯•

        Args:
            test_name: æµ‹è¯•åç§°
            scale: æµ‹è¯•è§„æ¨¡
            negatives: è´Ÿæ•°å‘ç¥¨åˆ—è¡¨
            batch_id: æ•°æ®æ‰¹æ¬¡ID

        Returns:
            PerformanceMetrics: æ€§èƒ½æŒ‡æ ‡
        """
        # é‡ç½®ç›‘æ§çŠ¶æ€
        monitor = get_monitor()
        monitor.reset_stats()

        # é‡ç½®å¹¶è·å–è¯¦ç»†æ€§èƒ½è®¡æ—¶å™¨
        reset_performance_timer()
        timer = get_performance_timer()

        # é¢„çƒ­ï¼šæ‰§è¡Œä¸€æ¬¡å°è§„æ¨¡æŸ¥è¯¢
        warmup_negatives = negatives[:5] if len(negatives) > 5 else negatives
        self.engine.match_batch(warmup_negatives, self.candidate_provider)
        time.sleep(1)  # ç­‰å¾…é¢„çƒ­å®Œæˆ

        # å¼€å§‹æ€§èƒ½ç›‘æ§
        start_memory = self.process.memory_info().rss / 1024 / 1024  # MB
        start_time = time.time()
        cpu_samples = []
        memory_samples = []

        # è®°å½•æ•°æ®åº“æŸ¥è¯¢å‰çŠ¶æ€
        db_query_start = time.time()

        # æ‰§è¡ŒåŒ¹é…æµ‹è¯•
        response_times = []

        # å°†è´Ÿæ•°å‘ç¥¨åˆ†æˆå¤šä¸ªå°æ‰¹æ¬¡ä»¥è·å¾—æ›´å¤šå“åº”æ—¶é—´æ ·æœ¬
        batch_size = min(100, len(negatives))  # æ¯æ‰¹æœ€å¤š100ä¸ª
        all_results = []

        with timer.measure("total_matching_process"):
            for i in range(0, len(negatives), batch_size):
                batch_negatives = negatives[i:i + batch_size]

                # è®°å½•å•æ‰¹æ¬¡æ€§èƒ½
                batch_start = time.time()

                # ç›‘æ§ç³»ç»Ÿèµ„æº
                cpu_samples.append(psutil.cpu_percent())
                memory_samples.append(self.process.memory_info().rss / 1024 / 1024)

                # æ‰§è¡ŒåŒ¹é…
                batch_results = self.engine.match_batch(
                    batch_negatives,
                    self.candidate_provider,
                    sort_strategy="amount_desc",
                    enable_monitoring=True
                )

                batch_time = (time.time() - batch_start) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                response_times.append(batch_time)
                all_results.extend(batch_results)

                # æ”¶é›†å¯è§£é‡Šæ€§åˆ†ææ•°æ®ï¼ˆå‡ ä¹é›¶æ€§èƒ½å¼€é”€ï¼‰
                if self.enable_explainability:
                    self.all_match_results.extend(batch_results)
                    self.all_negatives.extend(batch_negatives)

                # æ¯æ‰¹æ¬¡ä¹‹é—´çŸ­æš‚ä¼‘æ¯
                time.sleep(0.1)

        # æµ‹è¯•ç»“æŸ
        total_duration = time.time() - start_time
        db_query_time = (time.time() - db_query_start) * 1000  # æ¯«ç§’
        peak_memory = max(memory_samples) if memory_samples else start_memory
        avg_cpu = statistics.mean(cpu_samples) if cpu_samples else 0

        # è®¡ç®—å“åº”æ—¶é—´ç»Ÿè®¡
        if response_times:
            p50 = statistics.median(response_times)
            p90 = self._percentile(response_times, 90)
            p95 = self._percentile(response_times, 95)
            p99 = self._percentile(response_times, 99)
            avg_time = statistics.mean(response_times)
            max_time = max(response_times)
            min_time = min(response_times)
        else:
            p50 = p90 = p95 = p99 = avg_time = max_time = min_time = 0

        # è®¡ç®—åŒ¹é…ç»“æœç»Ÿè®¡
        success_count = sum(1 for r in all_results if r.success)
        success_rate = success_count / len(all_results) if all_results else 0
        total_matched = sum(r.total_matched for r in all_results)
        fragments = sum(r.fragments_created for r in all_results)

        # ä¸ºå¤±è´¥åˆ†æå‡†å¤‡æ•°æ®ï¼ˆä»…å½“å¯ç”¨å¯è§£é‡Šæ€§æ—¶ï¼‰
        if self.enable_explainability:
            self.current_test_failed_results = [r for r in all_results if not r.success]

        # è·å–è“ç¥¨è¡Œæ•°é‡ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
        batch_blue_lines_count, total_blue_lines_count = self._get_blue_lines_count(batch_id)

        # è·å–ç›‘æ§æ•°æ®
        health_report = monitor.get_health_report()
        technical_metrics = health_report.get('technical_metrics', {})
        performance_metrics = health_report.get('performance_metrics', {})

        # è·å–è¯¦ç»†æ€§èƒ½åˆ†è§£æ•°æ®
        performance_report = timer.get_performance_report()
        detailed_breakdown = {}
        for step_name, times in performance_report.step_timings.items():
            if times:
                detailed_breakdown[step_name] = sum(times)

        # è®¡ç®—å•æ¬¡æ€§èƒ½æŒ‡æ ‡
        total_queries = performance_metrics.get('total_requests', len(negatives))
        avg_single_query_time = db_query_time / max(total_queries, 1)
        avg_single_match_time = (total_duration * 1000) / len(negatives) if negatives else 0

        # æ„é€ æ€§èƒ½æŒ‡æ ‡å¯¹è±¡
        metrics = PerformanceMetrics(
            test_name=test_name,
            data_scale=scale,
            batch_blue_lines_count=batch_blue_lines_count,
            total_blue_lines_count=total_blue_lines_count,
            negative_invoices_count=len(negatives),

            # å“åº”æ—¶é—´æŒ‡æ ‡
            response_times=response_times,
            p50_response_time=p50,
            p90_response_time=p90,
            p95_response_time=p95,
            p99_response_time=p99,
            avg_response_time=avg_time,
            max_response_time=max_time,
            min_response_time=min_time,

            # å•æ¬¡æ€§èƒ½æŒ‡æ ‡
            avg_single_query_time_ms=avg_single_query_time,
            avg_single_match_time_ms=avg_single_match_time,

            # åŒ¹é…ç»“æœæŒ‡æ ‡
            success_rate=success_rate,
            total_matched_amount=float(total_matched),
            fragments_created=fragments,

            # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
            peak_memory_mb=peak_memory,
            avg_cpu_percent=avg_cpu,
            database_query_count=performance_metrics.get('total_requests', 0),
            database_query_time_ms=db_query_time,

            # è¯¦ç»†æ€§èƒ½åˆ†è§£æŒ‡æ ‡
            detailed_performance_breakdown=detailed_breakdown,

            # æ•°æ®åº“æ€§èƒ½æŒ‡æ ‡ï¼ˆç®€åŒ–ï¼‰
            index_scan_time_ms=db_query_time * 0.4,  # ä¼°ç®—40%æ—¶é—´ç”¨äºç´¢å¼•æ‰«æ
            transaction_commit_time_ms=db_query_time * 0.2,  # ä¼°ç®—20%æ—¶é—´ç”¨äºäº‹åŠ¡æäº¤

            # æµ‹è¯•é…ç½®
            batch_size=batch_size,
            sort_strategy="amount_desc",
            enable_monitoring=True,

            # æ—¶é—´æˆ³
            test_timestamp=datetime.now().isoformat(),
            duration_seconds=total_duration
        )

        return metrics

    def _percentile(self, data: List[float], percentile: float) -> float:
        """è®¡ç®—ç™¾åˆ†ä½æ•°"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = (percentile / 100) * (len(sorted_data) - 1)
        if index.is_integer():
            return sorted_data[int(index)]
        else:
            lower_index = int(index)
            upper_index = lower_index + 1
            weight = index - lower_index
            return sorted_data[lower_index] * (1 - weight) + sorted_data[upper_index] * weight

    def _get_blue_lines_count(self, batch_id: str) -> Tuple[int, int]:
        """è·å–æŒ‡å®šæ‰¹æ¬¡çš„è“ç¥¨è¡Œæ•°é‡å’Œæ€»æ•°æ®é‡"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                # è·å–æ‰¹æ¬¡æ•°æ®é‡
                cur.execute("SELECT COUNT(*) FROM blue_lines WHERE batch_id = %s", (batch_id,))
                batch_count = cur.fetchone()[0]

                # è·å–æ€»æ•°æ®é‡
                cur.execute("SELECT COUNT(*) FROM blue_lines")
                total_count = cur.fetchone()[0]

                return batch_count, total_count
        finally:
            self.db_manager.pool.putconn(conn)

    def reset_existing_data(self):
        """é‡ç½®ç°æœ‰æ•°æ®çŠ¶æ€ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰"""
        if not self.preserve_data:
            print("ğŸ”„ é‡ç½®ç°æœ‰æ•°æ®çŠ¶æ€...")
            try:
                from tests.test_data_generator import TestDataGenerator
                generator = TestDataGenerator(self.db_config)
                try:
                    # åªæ¸…ç†åŒ¹é…è®°å½•ï¼Œä¿ç•™è“ç¥¨è¡Œæ•°æ®å¹¶é‡ç½®å…¶çŠ¶æ€
                    generator.reset_test_data()
                    print("âœ… æ•°æ®çŠ¶æ€å·²é‡ç½®")
                except Exception as e:
                    print(f"âš ï¸  é‡ç½®æ•°æ®çŠ¶æ€å¤±è´¥: {e}")
                finally:
                    generator.close()
            except Exception as e:
                print(f"âš ï¸  é‡ç½®æ“ä½œå¤±è´¥: {e}")

    def cleanup_after_test(self):
        """æµ‹è¯•åçš„æ¸…ç†å·¥ä½œ"""
        if self.preserve_data:
            print("ğŸ”’ ä¿ç•™æµ‹è¯•æ•°æ®å’ŒçŠ¶æ€")
            return

        print("ğŸ”„ é‡ç½®æµ‹è¯•åçš„æ•°æ®çŠ¶æ€...")
        try:
            from tests.test_data_generator import TestDataGenerator
            generator = TestDataGenerator(self.db_config)
            try:
                # åªæ¸…ç†åŒ¹é…è®°å½•å¹¶é‡ç½®ä½™é¢ï¼Œä¿ç•™è“ç¥¨è¡Œæ•°æ®
                generator.reset_test_data()
                print("âœ… æ•°æ®çŠ¶æ€å·²é‡ç½®ï¼Œå¯é‡å¤ä½¿ç”¨")
            except Exception as e:
                print(f"âš ï¸  é‡ç½®å¤±è´¥: {e}")
            finally:
                generator.close()
        except Exception as e:
            print(f"âš ï¸  æ¸…ç†æ“ä½œå¤±è´¥: {e}")

    def get_data_utilization_before_test(self):
        """è·å–æµ‹è¯•å‰æ•°æ®åˆ©ç”¨ç‡"""
        from tests.test_data_generator import TestDataGenerator
        generator = TestDataGenerator(self.db_config)
        try:
            return generator.get_data_utilization_stats()
        finally:
            generator.close()

    def check_data_availability(self, required_remaining_ratio: float = 0.15):
        """
        æ£€æŸ¥æ•°æ®å¯ç”¨æ€§

        Args:
            required_remaining_ratio: è¦æ±‚çš„å‰©ä½™æ•°æ®æ¯”ä¾‹ï¼ˆé»˜è®¤15%ï¼Œé€‚åº”çœŸå®ä¸šåŠ¡åœºæ™¯ï¼‰

        Returns:
            bool: æ•°æ®æ˜¯å¦å……è¶³
        """
        stats = self.get_data_utilization_before_test()
        if not stats:
            return False

        total_util = stats.get('total_utilization_percent', 100)
        remaining_ratio = (100 - total_util) / 100

        print(f"ğŸ“Š æ•°æ®å¯ç”¨æ€§æ£€æŸ¥:")
        print(f"  å½“å‰åˆ©ç”¨ç‡: {total_util:.1f}%")
        print(f"  å‰©ä½™æ¯”ä¾‹: {remaining_ratio:.1%}")
        print(f"  è¦æ±‚æ¯”ä¾‹: {required_remaining_ratio:.1%}")

        is_sufficient = remaining_ratio >= required_remaining_ratio

        if not is_sufficient:
            print(f"âš ï¸  æ•°æ®ä¸è¶³ï¼å»ºè®®é‡ç½®æ•°æ®æˆ–é™ä½æµ‹è¯•è§„æ¨¡")
            print(f"  å¯ç”¨æ•°æ®: {stats.get('unused_lines', 0) + stats.get('partial_used_lines', 0):,} è¡Œ")
            print(f"  å»ºè®®æ“ä½œ: python tests/test_data_generator.py --reset-data")
        else:
            print(f"âœ… æ•°æ®å……è¶³ï¼Œå¯ä»¥è¿›è¡Œæµ‹è¯•")

        return is_sufficient

    def _print_test_summary(self, metrics: PerformanceMetrics):
        """æ‰“å°æµ‹è¯•ç»“æœæ‘˜è¦"""
        print(f"  âœ“ æµ‹è¯•å®Œæˆ: {metrics.test_name}")
        print(f"    æ•°æ®è§„æ¨¡: æµ‹è¯•æ‰¹æ¬¡ {metrics.batch_blue_lines_count:,} æ¡ / æ•°æ®åº“æ€»é‡ {metrics.total_blue_lines_count:,} æ¡")
        print(f"    è´Ÿæ•°å‘ç¥¨: {metrics.negative_invoices_count} ä¸ª")
        print(f"    åŒ¹é…æˆåŠŸç‡: {metrics.success_rate:.1%}")
        print(f"    æ‰¹é‡å“åº”æ—¶é—´: P50={metrics.p50_response_time:.1f}ms, P99={metrics.p99_response_time:.1f}ms")
        print(f"    å•æ¬¡æ€§èƒ½æŒ‡æ ‡:")
        print(f"      - å•æ¬¡æŸ¥è¯¢: {metrics.avg_single_query_time_ms:.1f}ms")
        print(f"      - å•ä¸ªåŒ¹é…: {metrics.avg_single_match_time_ms:.1f}ms")
        print(f"    å†…å­˜å³°å€¼: {metrics.peak_memory_mb:.1f}MB")
        print(f"    æ€»è€—æ—¶: {metrics.duration_seconds:.2f}ç§’")

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æ€§èƒ½ç›®æ ‡ï¼ˆåŸºäºå•ä¸ªåŒ¹é…æ—¶é—´ï¼‰
        if metrics.avg_single_match_time_ms <= 70:
            print(f"    ğŸ¯ å•æ¬¡åŒ¹é…æ€§èƒ½ç›®æ ‡è¾¾æˆ: {metrics.avg_single_match_time_ms:.1f}ms â‰¤ 70ms")
        else:
            print(f"    âš ï¸  å•æ¬¡åŒ¹é…æ€§èƒ½ç›®æ ‡æœªè¾¾æˆ: {metrics.avg_single_match_time_ms:.1f}ms > 70ms")

        if metrics.success_rate >= 0.93:
            print(f"    ğŸ¯ åŒ¹é…ç‡ç›®æ ‡è¾¾æˆ: {metrics.success_rate:.1%} â‰¥ 93%")
        else:
            print(f"    âš ï¸  åŒ¹é…ç‡ç›®æ ‡æœªè¾¾æˆ: {metrics.success_rate:.1%} < 93%")

        # æ˜¾ç¤ºå¤±è´¥åŸå› åˆ†æï¼ˆå¦‚æœå¯ç”¨äº†å¯è§£é‡Šæ€§ï¼‰
        if self.enable_explainability and hasattr(self, 'current_test_failed_results'):
            self._print_failure_analysis(self.current_test_failed_results)

        # æ˜¾ç¤ºè¯¦ç»†æ€§èƒ½åˆ†è§£ï¼ˆå¦‚æœæœ‰ï¼‰
        if metrics.detailed_performance_breakdown:
            print(f"    è¯¦ç»†æ€§èƒ½åˆ†è§£:")
            for step, time_ms in metrics.detailed_performance_breakdown.items():
                print(f"      - {step}: {time_ms:.1f}ms")

    def _print_failure_analysis(self, failed_results):
        """æ‰“å°å¤±è´¥åŸå› åˆ†æ"""
        if not failed_results:
            return

        print(f"    ğŸ’¡ å¤±è´¥åŸå› åˆ†æ ({len(failed_results)} ç¬”å¤±è´¥):")

        # ç»Ÿè®¡å¤±è´¥åŸå› 
        failure_reasons = {}
        for result in failed_results:
            reason = result.failure_reason or "unknown"
            failure_reasons[reason] = failure_reasons.get(reason, 0) + 1

        # æ˜¾ç¤ºTop3å¤±è´¥åŸå› 
        sorted_reasons = sorted(failure_reasons.items(), key=lambda x: x[1], reverse=True)
        for i, (reason, count) in enumerate(sorted_reasons[:3], 1):
            reason_desc = self._get_failure_reason_description(reason)
            percentage = count / len(failed_results) * 100
            print(f"      {i}. {reason_desc}: {count} ç¬” ({percentage:.1f}%)")

    def _get_failure_reason_description(self, reason_code: str) -> str:
        """è·å–å¤±è´¥åŸå› çš„ä¸­æ–‡æè¿°"""
        descriptions = {
            "no_candidates": "æ— å¯ç”¨å€™é€‰è“ç¥¨",
            "insufficient_total_amount": "å€™é€‰é›†æ€»é¢ä¸è¶³",
            "fragmentation_issue": "å€™é€‰é›†è¿‡åº¦ç¢ç‰‡åŒ–",
            "no_matching_tax_rate": "ç¨ç‡ä¸åŒ¹é…",
            "no_matching_buyer": "ä¹°æ–¹ä¸åŒ¹é…",
            "no_matching_seller": "å–æ–¹ä¸åŒ¹é…",
            "greedy_suboptimal": "ç®—æ³•ç­–ç•¥æ¬¡ä¼˜",
            "concurrent_conflict": "å¹¶å‘å†²çª",
            "insufficient_funds": "èµ„é‡‘ä¸è¶³ï¼ˆæ—§ç‰ˆï¼‰"
        }
        return descriptions.get(reason_code, f"æœªçŸ¥åŸå›  ({reason_code})")

    def _generate_explainability_report(self) -> str:
        """ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†ææŠ¥å‘Š"""
        if not self.all_match_results:
            return ""

        # ä½¿ç”¨å¯è§£é‡Šæ€§æŠ¥å‘Šå™¨åˆ†ææ‰€æœ‰ç»“æœ
        batch_analysis = self.explainability_reporter.generate_batch_analysis(
            self.all_match_results, self.all_negatives
        )

        report_lines = []
        report_lines.append("\n## åŒ¹é…å¤±è´¥åˆ†æï¼ˆå¯è§£é‡Šæ€§æŠ¥å‘Šï¼‰\n")

        if batch_analysis.failure_count == 0:
            report_lines.append("ğŸ‰ **æ‰€æœ‰è´Ÿæ•°å‘ç¥¨å‡åŒ¹é…æˆåŠŸï¼** ç³»ç»Ÿè¿è¡Œå®Œç¾ã€‚\n")
            return "".join(report_lines)

        # å¤±è´¥æ¦‚å†µ
        report_lines.append("### å¤±è´¥æ¦‚å†µ\n")
        report_lines.append(f"- **æ€»å¤„ç†é‡**: {batch_analysis.total_processed:,} ç¬”\n")
        report_lines.append(f"- **æˆåŠŸåŒ¹é…**: {batch_analysis.success_count:,} ç¬” ({batch_analysis.success_rate:.1%})\n")
        report_lines.append(f"- **åŒ¹é…å¤±è´¥**: {batch_analysis.failure_count:,} ç¬” ({100-batch_analysis.success_rate*100:.1f}%)\n\n")

        # å¤±è´¥åŸå› åˆ†å¸ƒ
        if batch_analysis.failure_patterns:
            report_lines.append("### å¤±è´¥åŸå› åˆ†å¸ƒ\n")
            report_lines.append("| å¤±è´¥åŸå›  | æ•°é‡ | å å¤±è´¥æ¯”ä¾‹ | å½±å“æè¿° |\n")
            report_lines.append("|----------|------|------------|----------|\n")

            total_failures = batch_analysis.failure_count
            for reason, count in batch_analysis.failure_patterns.items():
                percentage = count / total_failures * 100
                reason_desc = self._get_failure_reason_description(reason)
                impact_desc = self._get_failure_impact_description(reason)
                report_lines.append(f"| {reason_desc} | {count} | {percentage:.1f}% | {impact_desc} |\n")
            report_lines.append("\n")

        # ä¸šåŠ¡å½±å“åˆ†æ
        impact = batch_analysis.business_impact_summary
        if impact and impact.get('total_failed_amount', 0) > 0:
            report_lines.append("### ä¸šåŠ¡å½±å“åˆ†æ\n")
            report_lines.append(f"- **å¤±è´¥æ€»é‡‘é¢**: Â¥{impact['total_failed_amount']:,.2f}\n")
            report_lines.append(f"- **å¹³å‡å¤±è´¥é‡‘é¢**: Â¥{impact.get('avg_failure_amount', 0):.2f}\n")

            if impact.get('high_value_failures', 0) > 0:
                report_lines.append(f"- **é«˜ä»·å€¼å¤±è´¥**: {impact['high_value_failures']} ç¬”ï¼ˆ>Â¥10,000ï¼‰âš ï¸\n")

            # å¤±è´¥åˆ†å¸ƒ
            if 'failure_by_amount_range' in impact:
                ranges = impact['failure_by_amount_range']
                report_lines.append("- **å¤±è´¥åˆ†å¸ƒ**:\n")
                report_lines.append(f"  - å°é¢(<Â¥100): {ranges.get('small', 0)} ç¬”\n")
                report_lines.append(f"  - ä¸­é¢(Â¥100-1K): {ranges.get('medium', 0)} ç¬”\n")
                report_lines.append(f"  - å¤§é¢(>Â¥1K): {ranges.get('large', 0)} ç¬”\n")
            report_lines.append("\n")

        # æ”¹è¿›å»ºè®®
        if batch_analysis.recommendations:
            report_lines.append("### é’ˆå¯¹æ€§æ”¹è¿›å»ºè®®\n")
            for i, recommendation in enumerate(batch_analysis.recommendations, 1):
                report_lines.append(f"{i}. {recommendation}\n")
            report_lines.append("\n")

        # æ·±åº¦è¯Šæ–­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_deep_diagnosis:
            report_lines.append("### æ·±åº¦è¯Šæ–­åˆ†æ\n")
            report_lines.append("åŸºäºå¯ç”¨çš„æ·±åº¦è¯Šæ–­åŠŸèƒ½ï¼Œä»¥ä¸‹æ˜¯è¯¦ç»†åˆ†æï¼š\n\n")

            # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§å¤±è´¥æ¡ˆä¾‹è¿›è¡Œæ·±åº¦åˆ†æ
            failed_results = [r for r in self.all_match_results if not r.success]
            sample_failures = failed_results[:5]  # åˆ†æå‰5ä¸ªå¤±è´¥æ¡ˆä¾‹

            for result in sample_failures:
                matching_negative = next((n for n in self.all_negatives if n.invoice_id == result.negative_invoice_id), None)
                if matching_negative:
                    try:
                        diagnosis = self.diagnostics.diagnose_no_match(matching_negative)
                        report_lines.append(f"**æ¡ˆä¾‹ #{result.negative_invoice_id}**:\n")
                        report_lines.append(f"- ä¸»è¦é—®é¢˜: {diagnosis.primary_issue}\n")
                        report_lines.append(f"- ç½®ä¿¡åº¦: {diagnosis.confidence_score:.1%}\n")
                        if diagnosis.alternative_solutions:
                            report_lines.append(f"- å»ºè®®: {diagnosis.alternative_solutions[0]}\n")
                        report_lines.append("\n")
                    except Exception as e:
                        report_lines.append(f"**æ¡ˆä¾‹ #{result.negative_invoice_id}**: è¯Šæ–­åˆ†æå¤±è´¥ ({str(e)})\n")

        return "".join(report_lines)

    def _get_failure_impact_description(self, reason_code: str) -> str:
        """è·å–å¤±è´¥åŸå› çš„å½±å“æè¿°"""
        impact_descriptions = {
            "no_candidates": "æ•°æ®æµé—®é¢˜ï¼Œå½±å“å¤„ç†æ•ˆç‡",
            "insufficient_total_amount": "èµ„é‡‘è°ƒé…é—®é¢˜ï¼Œå¯èƒ½éœ€è¦æ‹†åˆ†",
            "fragmentation_issue": "æ•°æ®è´¨é‡é—®é¢˜ï¼Œå½±å“ç³»ç»Ÿæ€§èƒ½",
            "no_matching_tax_rate": "ä¸šåŠ¡è§„åˆ™ä¸¥æ ¼ï¼Œéœ€è¦äººå·¥å®¡æ ¸",
            "no_matching_buyer": "æ•°æ®ä¸€è‡´æ€§é—®é¢˜",
            "no_matching_seller": "æ•°æ®ä¸€è‡´æ€§é—®é¢˜",
            "greedy_suboptimal": "ç®—æ³•ä¼˜åŒ–ç©ºé—´",
            "concurrent_conflict": "ç³»ç»Ÿå¹¶å‘é—®é¢˜",
            "insufficient_funds": "å†å²é—ç•™é—®é¢˜"
        }
        return impact_descriptions.get(reason_code, "éœ€è¦è¿›ä¸€æ­¥åˆ†æ")

    def generate_performance_report(self, results: List[PerformanceMetrics],
                                  output_file: Optional[str] = None) -> str:
        """
        ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š

        Args:
            results: æµ‹è¯•ç»“æœåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            str: æŠ¥å‘Šå†…å®¹
        """
        if not results:
            return "æ— æµ‹è¯•ç»“æœ"

        # è·å–ç³»ç»Ÿä¿¡æ¯
        system_info = {
            'cpu_count': psutil.cpu_count(),
            'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {},
            'memory_total_gb': psutil.virtual_memory().total / 1024 / 1024 / 1024,
            'python_version': sys.version,
            'postgresql_version': self._get_postgresql_version()
        }

        # ç”ŸæˆæŠ¥å‘Š
        report = self._format_performance_report(results, system_info)

        # ä¿å­˜åˆ°æ–‡ä»¶
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"âœ“ æ€§èƒ½æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_file}")

        return report

    def _get_postgresql_version(self) -> str:
        """è·å–PostgreSQLç‰ˆæœ¬"""
        conn = self.db_manager.pool.getconn()
        try:
            with conn.cursor() as cur:
                cur.execute("SELECT version()")
                return cur.fetchone()[0]
        except:
            return "Unknown"
        finally:
            self.db_manager.pool.putconn(conn)

    def _format_performance_report(self, results: List[PerformanceMetrics],
                                 system_info: Dict) -> str:
        """æ ¼å¼åŒ–æ€§èƒ½æŠ¥å‘Š"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

        report = f"""
# è´Ÿæ•°å‘ç¥¨åŒ¹é…ç³»ç»Ÿ - å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚å†µ
- **æµ‹è¯•æ—¶é—´**: {timestamp}
- **æµ‹è¯•ç›®æ ‡**: éªŒè¯P99å»¶è¿Ÿ<70msï¼ŒåŒ¹é…ç‡>93%
- **æµ‹è¯•ç”¨ä¾‹æ•°**: {len(results)}

## æµ‹è¯•ç¯å¢ƒ

### ç¡¬ä»¶é…ç½®
- **CPU**: {system_info['cpu_count']} æ ¸å¿ƒ
- **å†…å­˜**: {system_info['memory_total_gb']:.1f} GB
- **å­˜å‚¨**: SSD (300GB)

### è½¯ä»¶é…ç½®
- **æ•°æ®åº“**: PostgreSQL 17.6
- **Python**: {system_info['python_version'].split()[0]}
- **æ•°æ®åº“ç‰ˆæœ¬**: {system_info['postgresql_version']}

## æµ‹è¯•ç»“æœæ±‡æ€»

### å…³é”®æŒ‡æ ‡è¾¾æˆæƒ…å†µ
"""

        # æ£€æŸ¥å…³é”®æŒ‡æ ‡è¾¾æˆæƒ…å†µ
        p99_passed = sum(1 for r in results if r.p99_response_time <= 70)
        success_rate_passed = sum(1 for r in results if r.success_rate >= 0.93)

        report += f"""
| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¾¾æˆç‡ | çŠ¶æ€ |
|------|--------|--------|------|
| P99å“åº”æ—¶é—´ | â‰¤70ms | {p99_passed}/{len(results)} ({p99_passed/len(results):.1%}) | {'âœ…' if p99_passed == len(results) else 'âš ï¸'} |
| åŒ¹é…æˆåŠŸç‡ | â‰¥93% | {success_rate_passed}/{len(results)} ({success_rate_passed/len(results):.1%}) | {'âœ…' if success_rate_passed == len(results) else 'âš ï¸'} |

### è¯¦ç»†æµ‹è¯•ç»“æœ

"""

        # è¯¦ç»†ç»“æœè¡¨æ ¼
        report += "| æµ‹è¯•è§„æ¨¡ | æµ‹è¯•æ‰¹æ¬¡æ•°æ® | æ•°æ®åº“æ€»é‡ | è´Ÿæ•°å‘ç¥¨æ•° | å•æ¬¡æŸ¥è¯¢(ms) | å•ä¸ªåŒ¹é…(ms) | P99æ‰¹é‡(ms) | åŒ¹é…ç‡ | å†…å­˜å³°å€¼(MB) | æ€»è€—æ—¶(s) |\n"
        report += "|----------|-------------|------------|------------|-------------|-------------|-------------|---------|-------------|----------|\n"

        for r in results:
            report += f"| {r.data_scale} | {r.batch_blue_lines_count:,} | {r.total_blue_lines_count:,} | {r.negative_invoices_count} | "
            report += f"{r.avg_single_query_time_ms:.1f} | {r.avg_single_match_time_ms:.1f} | "
            report += f"{r.p99_response_time:.1f} | {r.success_rate:.1%} | {r.peak_memory_mb:.1f} | {r.duration_seconds:.2f} |\n"

        # æ€§èƒ½åˆ†æ
        report += "\n## æ€§èƒ½åˆ†æ\n\n"

        # æœ€ä½³å’Œæœ€å·®æ€§èƒ½
        best_p99 = min(results, key=lambda x: x.p99_response_time)
        worst_p99 = max(results, key=lambda x: x.p99_response_time)
        best_success = max(results, key=lambda x: x.success_rate)
        worst_success = min(results, key=lambda x: x.success_rate)

        # è®¡ç®—å•æ¬¡æ€§èƒ½æŒ‡æ ‡
        best_single_query = min(results, key=lambda x: x.avg_single_query_time_ms)
        best_single_match = min(results, key=lambda x: x.avg_single_match_time_ms)

        report += f"""### æ€§èƒ½è¡¨ç°
- **å•æ¬¡æŸ¥è¯¢æ€§èƒ½**: å¹³å‡{best_single_query.avg_single_query_time_ms:.1f}ms (æ–¹ä¾¿æ’æŸ¥)
- **å•æ¬¡åŒ¹é…æ€§èƒ½**: å¹³å‡{best_single_match.avg_single_match_time_ms:.1f}ms (ç¬¦åˆ<70msç›®æ ‡)
- **æœ€ä½³P99æ‰¹å¤„ç†æ—¶é—´**: {best_p99.p99_response_time:.1f}ms ({best_p99.test_name})
- **æœ€å·®P99æ‰¹å¤„ç†æ—¶é—´**: {worst_p99.p99_response_time:.1f}ms ({worst_p99.test_name})
- **æœ€é«˜åŒ¹é…æˆåŠŸç‡**: {best_success.success_rate:.1%} ({best_success.test_name})
- **æœ€ä½åŒ¹é…æˆåŠŸç‡**: {worst_success.success_rate:.1%} ({worst_success.test_name})

### èµ„æºä½¿ç”¨åˆ†æ
"""

        max_memory = max(r.peak_memory_mb for r in results)
        avg_memory = sum(r.peak_memory_mb for r in results) / len(results)
        max_cpu = max(r.avg_cpu_percent for r in results)

        report += f"""- **å†…å­˜å³°å€¼**: {max_memory:.1f}MB
- **å¹³å‡å†…å­˜ä½¿ç”¨**: {avg_memory:.1f}MB
- **CPUå³°å€¼**: {max_cpu:.1f}%

### å¯æ‰©å±•æ€§åˆ†æ
"""

        # æŒ‰è§„æ¨¡åˆ†ç»„åˆ†æ
        scale_groups = {}
        for r in results:
            if r.data_scale not in scale_groups:
                scale_groups[r.data_scale] = []
            scale_groups[r.data_scale].append(r)

        for scale, scale_results in scale_groups.items():
            avg_p99 = sum(r.p99_response_time for r in scale_results) / len(scale_results)
            avg_success = sum(r.success_rate for r in scale_results) / len(scale_results)
            data_size = scale_results[0].batch_blue_lines_count

            report += f"- **{scale}è§„æ¨¡** ({data_size:,}æ¡æ•°æ®): å¹³å‡P99={avg_p99:.1f}ms, å¹³å‡åŒ¹é…ç‡={avg_success:.1%}\n"

        # ç»“è®ºå’Œå»ºè®®
        report += "\n## ç»“è®ºä¸å»ºè®®\n\n"

        overall_p99_pass = p99_passed == len(results)
        overall_success_pass = success_rate_passed == len(results)

        if overall_p99_pass and overall_success_pass:
            report += "âœ… **æ€»ä½“è¯„ä¼°**: ç³»ç»Ÿæ€§èƒ½å®Œå…¨æ»¡è¶³è®¾è®¡ç›®æ ‡ï¼Œå¯ä»¥æ”¯æ’‘ç”Ÿäº§ç¯å¢ƒè¿è¡Œã€‚\n\n"
        elif overall_p99_pass or overall_success_pass:
            report += "âš ï¸ **æ€»ä½“è¯„ä¼°**: ç³»ç»Ÿæ€§èƒ½éƒ¨åˆ†æ»¡è¶³è®¾è®¡ç›®æ ‡ï¼Œå»ºè®®é’ˆå¯¹æ€§ä¼˜åŒ–ã€‚\n\n"
        else:
            report += "âŒ **æ€»ä½“è¯„ä¼°**: ç³»ç»Ÿæ€§èƒ½æœªè¾¾åˆ°è®¾è®¡ç›®æ ‡ï¼Œéœ€è¦é‡å¤§ä¼˜åŒ–ã€‚\n\n"

        # å…·ä½“å»ºè®®
        report += "### ä¼˜åŒ–å»ºè®®\n"

        if not overall_p99_pass:
            report += "- **æ€§èƒ½ä¼˜åŒ–**: P99å“åº”æ—¶é—´è¶…æ ‡ï¼Œå»ºè®®ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢å’Œç´¢å¼•ç­–ç•¥\n"

        if not overall_success_pass:
            report += "- **ç®—æ³•ä¼˜åŒ–**: åŒ¹é…æˆåŠŸç‡ä¸è¾¾æ ‡ï¼Œå»ºè®®è°ƒæ•´è´ªå©ªç®—æ³•å‚æ•°æˆ–å€™é€‰é›†å¤§å°\n"

        if max_memory > 2000:  # 2GB
            report += "- **å†…å­˜ä¼˜åŒ–**: å†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®ä½¿ç”¨æµå¼å¤„ç†å‡å°‘å†…å­˜å ç”¨\n"

        report += "- **ç›‘æ§å»ºè®®**: å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­éƒ¨ç½²å®æ—¶æ€§èƒ½ç›‘æ§\n"
        report += "- **å®¹é‡è§„åˆ’**: åŸºäºæµ‹è¯•ç»“æœåˆ¶å®šåˆç†çš„å®¹é‡è§„åˆ’ç­–ç•¥\n"

        # å¯è§£é‡Šæ€§åˆ†æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_explainability and hasattr(self, 'all_match_results') and self.all_match_results:
            explainability_section = self._generate_explainability_report()
            report += explainability_section

        # JSONæ•°æ®ï¼ˆç”¨äºè¿›ä¸€æ­¥åˆ†æï¼‰
        report += "\n## åŸå§‹æ•°æ®\n\n"
        report += "```json\n"
        json_data = {
            'system_info': system_info,
            'test_results': [asdict(r) for r in results],
            'summary': {
                'total_tests': len(results),
                'p99_target_achieved': p99_passed,
                'success_rate_target_achieved': success_rate_passed,
                'best_p99_ms': best_p99.p99_response_time,
                'worst_p99_ms': worst_p99.p99_response_time,
                'best_success_rate': best_success.success_rate,
                'worst_success_rate': worst_success.success_rate
            }
        }
        report += json.dumps(json_data, indent=2, ensure_ascii=False)
        report += "\n```\n"

        return report

    def cleanup_generated_batches(self):
        """æ¸…ç†æœ¬æ¬¡æµ‹è¯•ç”Ÿæˆçš„æ•°æ®æ‰¹æ¬¡ï¼ˆä»…åœ¨ä¸ä¿ç•™æ•°æ®æ—¶ï¼‰"""
        if self.preserve_data or not self.test_batch_ids:
            return

        print(f"\nğŸ—‘ï¸ æ¸…ç†æœ¬æ¬¡æµ‹è¯•ç”Ÿæˆçš„æ•°æ®æ‰¹æ¬¡...")
        for batch_id in self.test_batch_ids:
            try:
                print(f"æ¸…ç†æ‰¹æ¬¡: {batch_id}")
                self.data_generator.clear_batch(batch_id)
            except Exception as e:
                print(f"âš ï¸ æ¸…ç†æ‰¹æ¬¡ {batch_id} å¤±è´¥: {e}")

        self.test_batch_ids.clear()
        print("âœ… æ•°æ®æ‰¹æ¬¡æ¸…ç†å®Œæˆ")

    def close(self):
        """å…³é—­èµ„æº"""
        self.data_generator.close()


def run_performance_tests(scales: List[str], cleanup: bool = True,
                         report_file: Optional[str] = None, preserve_data: bool = False,
                         delete_data: bool = False, enable_explainability: bool = True,
                         enable_deep_diagnosis: bool = False, seed: Optional[int] = None,
                         debug_mode: bool = False):
    """
    è¿è¡Œæ€§èƒ½æµ‹è¯•

    Args:
        scales: æµ‹è¯•è§„æ¨¡åˆ—è¡¨
        cleanup: æ˜¯å¦æ¸…ç†æµ‹è¯•æ•°æ®
        report_file: æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶
        preserve_data: æ˜¯å¦ä¿ç•™æµ‹è¯•åçš„æ•°æ®ï¼ˆé»˜è®¤Falseï¼Œæµ‹è¯•åä¼šæ¢å¤æ•°æ®ï¼‰
        delete_data: æ˜¯å¦åˆ é™¤æµ‹è¯•æ•°æ®ï¼ˆé»˜è®¤Falseï¼Œåªé‡ç½®çŠ¶æ€ä¿ç•™æ•°æ®ä»¥ä¾¿å¤ç”¨ï¼‰
        enable_explainability: æ˜¯å¦å¯ç”¨å¯è§£é‡Šæ€§åˆ†æï¼ˆé»˜è®¤Trueï¼Œå‡ ä¹æ— æ€§èƒ½å½±å“ï¼‰
        enable_deep_diagnosis: æ˜¯å¦å¯ç”¨æ·±åº¦è¯Šæ–­ï¼ˆé»˜è®¤Falseï¼Œå¯é€‰æ‹©æ€§å¯ç”¨ï¼‰
        seed: éšæœºç§å­ï¼ˆå¯é€‰ï¼Œç”¨äºç”Ÿæˆå¯é‡å¤çš„æµ‹è¯•æ•°æ®ï¼‰
    """
    print("=== è´Ÿæ•°å‘ç¥¨åŒ¹é…ç³»ç»Ÿ - å¤§è§„æ¨¡æ€§èƒ½æµ‹è¯• ===\n")

    # æ˜¾ç¤ºå¯è§£é‡Šæ€§åŠŸèƒ½çŠ¶æ€
    if enable_explainability:
        print("ğŸ” å¯è§£é‡Šæ€§åˆ†æ: å·²å¯ç”¨")
        if enable_deep_diagnosis:
            print("ğŸ”¬ æ·±åº¦è¯Šæ–­: å·²å¯ç”¨ï¼ˆå¯èƒ½ç¨å¾®å½±å“æ€§èƒ½ï¼‰")
        else:
            print("ğŸ”¬ æ·±åº¦è¯Šæ–­: æœªå¯ç”¨ï¼ˆå¯é€šè¿‡ --enable-deep-diagnosis å¯ç”¨ï¼‰")
    else:
        print("ğŸ” å¯è§£é‡Šæ€§åˆ†æ: å·²ç¦ç”¨")
    print()

    # æ˜¾ç¤ºéšæœºç§å­çŠ¶æ€
    if seed is not None:
        print(f"ğŸŒ± ä½¿ç”¨å›ºå®šéšæœºç§å­: {seed}")
        print("   æ³¨æ„: ç›¸åŒç§å­å°†ç”Ÿæˆå®Œå…¨ç›¸åŒçš„æµ‹è¯•æ•°æ®\n")
    else:
        print("ğŸ² ä½¿ç”¨éšæœºæ•°æ®ç”Ÿæˆ (æ¯æ¬¡è¿è¡Œç»“æœå¯èƒ½ä¸åŒ)\n")

    # åˆå§‹åŒ–æµ‹è¯•å¥—ä»¶
    db_config = get_db_config('test')
    test_suite = PerformanceTestSuite(
        db_config,
        preserve_data=preserve_data,
        enable_explainability=enable_explainability,
        enable_deep_diagnosis=enable_deep_diagnosis,
        seed=seed
    )

    # è®¾ç½®è°ƒè¯•æ¨¡å¼
    test_suite.engine.debug_mode = debug_mode

    all_results = []
    batch_ids = []

    try:
        # æ£€æŸ¥æ•°æ®å¯ç”¨æ€§
        print("ğŸ“Š æ£€æŸ¥æ•°æ®å¯ç”¨æ€§...")
        if not test_suite.check_data_availability():
            print("âŒ æ•°æ®ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
            return

        # é‡ç½®ç°æœ‰æ•°æ®çŠ¶æ€
        # æ³¨æ„ï¼šå½“ä½¿ç”¨å›ºå®šç§å­æ—¶ï¼Œå¿…é¡»é‡ç½®æ•°æ®çŠ¶æ€ä»¥ç¡®ä¿å¯é‡å¤æ€§
        if seed is not None:
            print("ğŸ”„ æ£€æµ‹åˆ°å›ºå®šç§å­ï¼Œå¼ºåˆ¶é‡ç½®æ•°æ®çŠ¶æ€ä»¥ç¡®ä¿å¯é‡å¤æ€§...")
            test_suite.preserve_data = False  # ä¸´æ—¶è¦†ç›–è®¾ç½®
            test_suite.reset_existing_data()
            test_suite.preserve_data = preserve_data  # æ¢å¤åŸè®¾ç½®
        else:
            # ä»…åœ¨ä¸ä¿ç•™æ•°æ®æ—¶é‡ç½®
            test_suite.reset_existing_data()

        for scale in scales:
            print(f"\n{'='*60}")
            print(f"å¼€å§‹ {scale} è§„æ¨¡æµ‹è¯•")
            print(f"{'='*60}")

            # è®¾ç½®æµ‹è¯•æ•°æ®
            batch_id = test_suite.setup_test_data(scale)
            batch_ids.append(batch_id)

            # è¿è¡Œæ€§èƒ½æµ‹è¯•
            results = test_suite.run_performance_test(scale, batch_id)
            all_results.extend(results)

            print(f"\nâœ“ {scale} è§„æ¨¡æµ‹è¯•å®Œæˆ")

        # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        print(f"\n{'='*60}")
        print("ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print(f"{'='*60}")

        if not report_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            report_file = f"docs/reports/performance_test_report_{timestamp}.md"

        report = test_suite.generate_performance_report(all_results, report_file)

        # è¾“å‡ºç®€è¦ç»“æœ
        print("\nğŸ“Š æµ‹è¯•ç»“æœæ‘˜è¦:")
        p99_passed = sum(1 for r in all_results if r.p99_response_time <= 70)
        success_rate_passed = sum(1 for r in all_results if r.success_rate >= 0.93)

        print(f"  æ€»æµ‹è¯•ç”¨ä¾‹: {len(all_results)}")
        print(f"  P99ç›®æ ‡è¾¾æˆ: {p99_passed}/{len(all_results)} ({p99_passed/len(all_results):.1%})")
        print(f"  åŒ¹é…ç‡ç›®æ ‡è¾¾æˆ: {success_rate_passed}/{len(all_results)} ({success_rate_passed/len(all_results):.1%})")

        if p99_passed == len(all_results) and success_rate_passed == len(all_results):
            print("  ğŸ‰ æ‰€æœ‰æ€§èƒ½ç›®æ ‡è¾¾æˆï¼")
        else:
            print("  âš ï¸  éƒ¨åˆ†æ€§èƒ½ç›®æ ‡æœªè¾¾æˆï¼Œè¯·æŸ¥çœ‹è¯¦ç»†æŠ¥å‘Š")

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise
    finally:
        # æµ‹è¯•åæ¸…ç†å·¥ä½œ
        test_suite.cleanup_after_test()

        # æ¸…ç†æœ¬æ¬¡æµ‹è¯•ç”Ÿæˆçš„æ•°æ®æ‰¹æ¬¡ï¼ˆä»…åœ¨æ˜ç¡®è¦æ±‚åˆ é™¤æ—¶ï¼‰
        # é»˜è®¤è¡Œä¸ºï¼šä¿ç•™æ•°æ®ï¼Œåªé‡ç½®çŠ¶æ€ï¼ˆé™¤éæ˜ç¡®ä½¿ç”¨--delete-dataï¼‰
        if delete_data:
            test_suite.cleanup_generated_batches()

        test_suite.close()


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='è´Ÿæ•°å‘ç¥¨åŒ¹é…ç³»ç»Ÿå¤§è§„æ¨¡æ€§èƒ½æµ‹è¯•',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å°è§„æ¨¡æµ‹è¯•ï¼ˆé»˜è®¤ä½¿ç”¨ç§å­42ï¼Œå¯é‡å¤ï¼‰
  python test_performance_scale.py --scale small

  # ä½¿ç”¨éšæœºæ•°æ®ï¼ˆæ¯æ¬¡ç»“æœä¸åŒï¼‰
  python test_performance_scale.py --scale small --random

  # ä½¿ç”¨è‡ªå®šä¹‰å›ºå®šç§å­è¿›è¡Œå¯é‡å¤æµ‹è¯•
  python test_performance_scale.py --scale large --seed 12345

  # å¯¹æ¯”ä¼˜åŒ–å‰åæ€§èƒ½ï¼ˆé»˜è®¤ç§å­42ç¡®ä¿æ•°æ®ä¸€è‡´ï¼‰
  python test_performance_scale.py --scale large --report before_optimization.md
  # ... åº”ç”¨ä¼˜åŒ– ...
  python test_performance_scale.py --scale large --report after_optimization.md

  # æ‰€æœ‰è§„æ¨¡æµ‹è¯•
  python test_performance_scale.py --scale all

  # è‡ªå®šä¹‰æŠ¥å‘Šæ–‡ä»¶å
  python test_performance_scale.py --scale medium --report performance_2025.md

  # ä¿ç•™æµ‹è¯•æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
  python test_performance_scale.py --scale small --no-cleanup
        """
    )

    parser.add_argument('--scale',
                       choices=['small', 'medium', 'large', 'all'],
                       default='small',
                       help='æµ‹è¯•è§„æ¨¡ (é»˜è®¤: small)')

    parser.add_argument('--report', type=str,
                       help='æ€§èƒ½æŠ¥å‘Šæ–‡ä»¶è·¯å¾„ (é»˜è®¤: docs/reports/performance_test_report_TIMESTAMP.md)')

    parser.add_argument('--no-cleanup', action='store_true',
                       help='ä¸æ¸…ç†æµ‹è¯•æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰')

    parser.add_argument('--preserve-data', action='store_true',
                       help='ä¿ç•™æµ‹è¯•åçš„æ•°æ®çŠ¶æ€ï¼ˆä¸æ¢å¤åˆ°å¿«ç…§ï¼‰')

    parser.add_argument('--delete-data', action='store_true',
                       help='åˆ é™¤æµ‹è¯•ç”Ÿæˆçš„æ•°æ®ï¼ˆé»˜è®¤åªé‡ç½®çŠ¶æ€ï¼Œä¿ç•™æ•°æ®ä»¥ä¾¿å¤ç”¨ï¼‰')

    parser.add_argument('--disable-explainability', action='store_true',
                       help='ç¦ç”¨å¯è§£é‡Šæ€§åˆ†æï¼ˆå¾®å°æ€§èƒ½æå‡ï¼‰')

    parser.add_argument('--enable-deep-diagnosis', action='store_true',
                       help='å¯ç”¨æ·±åº¦è¯Šæ–­åˆ†æï¼ˆè¯¦ç»†å¤±è´¥åŸå› åˆ†æï¼Œå¯èƒ½ç¨å¾®å½±å“æ€§èƒ½ï¼‰')

    parser.add_argument('--seed', type=int, default=999,
                       help='éšæœºç§å­ï¼ˆé»˜è®¤: 999ï¼Œç”¨äºç”Ÿæˆå¯é‡å¤çš„æµ‹è¯•æ•°æ®ï¼‰ã€‚æ³¨æ„ï¼šä½¿ç”¨seedæ—¶ä¼šè‡ªåŠ¨é‡ç½®æ•°æ®çŠ¶æ€ä»¥ç¡®ä¿å¯é‡å¤æ€§')

    parser.add_argument('--random', action='store_true',
                       help='ä½¿ç”¨çœŸæ­£çš„éšæœºæ•°æ®ï¼ˆç¦ç”¨é»˜è®¤ç§å­999ï¼‰')

    parser.add_argument('--debug', action='store_true',
                       help='å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆè¯¦ç»†æ€§èƒ½ç»Ÿè®¡è¾“å‡ºï¼Œä¼šå½±å“æ€§èƒ½ï¼‰')

    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    # ç¡®å®šæµ‹è¯•è§„æ¨¡
    if args.scale == 'all':
        scales = ['small', 'medium', 'large']
    else:
        scales = [args.scale]

    # å¤„ç†ç§å­å‚æ•°
    seed = None if args.random else args.seed

    # è¿è¡Œæµ‹è¯•
    run_performance_tests(
        scales=scales,
        cleanup=not args.no_cleanup,
        report_file=args.report,
        preserve_data=args.preserve_data,
        delete_data=args.delete_data,
        enable_explainability=not args.disable_explainability,
        enable_deep_diagnosis=args.enable_deep_diagnosis,
        seed=seed,
        debug_mode=args.debug
    )